{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentimental Analysis for reviews related to British Airways\n",
    "\n",
    "**Executive Summary**\n",
    "\n",
    "British Airways is a major global airline that carries millions of passengers each year. The company collects a vast amount of feedback from its customers, both positive and negative. This feedback is valuable for understanding customer satisfaction and identifying areas for improvement. However, manually analyzing this feedback is a time-consuming and resource-intensive process.\n",
    "\n",
    "Sentiment analysis is a technique that can be used to automatically analyze customer feedback and identify the overall sentiment of the feedback. This information can then be used to:\n",
    "\n",
    "* Track changes in customer sentiment over time\n",
    "* Identify areas where British Airways is excelling or falling short\n",
    "* Develop targeted marketing campaigns\n",
    "* Improve customer satisfaction\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "The objectives of this project are to:\n",
    "\n",
    "* Collect a large corpus of British Airways reviews\n",
    "* Perform sentiment analysis on the reviews to identify the overall sentiment\n",
    "* Analyze the sentiment of the reviews by topic\n",
    "* Identify key themes and trends in the reviews\n",
    "* Develop recommendations for British Airways based on the findings of the analysis\n",
    "\n",
    "**Scope**\n",
    "\n",
    "The scope of this project will be limited to reviews of British Airways written in English. The reviews will be collected from online review sites, and travel forums.\n",
    "\n",
    "**Methodology**\n",
    "\n",
    "The following methodology will be used to conduct the sentiment analysis:\n",
    "\n",
    "1. **Data collection:** A web scraping tool will be used to collect a large corpus of British Airways reviews from a variety of sources.\n",
    "2. **Data cleaning:** The collected reviews will be cleaned to remove irrelevant information, such as HTML tags and punctuation.\n",
    "3. **Sentiment analysis:** A sentiment analysis tool will be used to analyze the sentiment of the reviews.\n",
    "4. **Topic modeling:** Topic modeling will be used to identify the key themes and trends in the reviews.\n",
    "\n",
    "\n",
    "**Resources**\n",
    "\n",
    "The following resources will be needed to complete the project:\n",
    "\n",
    "* A web scraping tool\n",
    "* A sentiment analysis tool\n",
    "* A topic modeling tool\n",
    "\n",
    "\n",
    "**Conclusion**\n",
    "\n",
    "The results of this project will provide British Airways with valuable insights into customer sentiment. This information can then be used to improve customer satisfaction and make informed business decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLaVGrhWDq3T"
   },
   "source": [
    "## 1. Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit the site you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pUvkZmf0Dq3Y"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ecfDDBBBDq3Z",
    "outputId": "74be6e79-f384-4078-cce4-0f6087e4ff2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "   ---> 1000 total reviews\n",
      "Scraping page 2\n",
      "   ---> 2000 total reviews\n",
      "Scraping page 3\n",
      "   ---> 3000 total reviews\n",
      "Scraping page 4\n",
      "   ---> 3713 total reviews\n",
      "Scraping page 5\n",
      "   ---> 3713 total reviews\n",
      "Scraping page 6\n",
      "   ---> 3713 total reviews\n",
      "Scraping page 7\n",
      "   ---> 3713 total reviews\n",
      "Scraping page 8\n",
      "   ---> 3713 total reviews\n",
      "Scraping page 9\n",
      "   ---> 3713 total reviews\n",
      "Scraping page 10\n",
      "   ---> 3713 total reviews\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 10\n",
    "page_size = 1000\n",
    "\n",
    "reviews = []\n",
    "\n",
    "# for i in range(1, pages + 1):\n",
    "for i in range(1, pages + 1):\n",
    "\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "\n",
    "    print(f\"   ---> {len(reviews)} total reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tm11eIIJDq3a",
    "outputId": "0a8d9310-6603-4c5d-e6eb-333731b63763"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified | Once again a terrible busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>✅ Trip Verified |  BA A380's are showing their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified |   Credit to BA - Flew to Sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅ Trip Verified |   The check in area for prem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified |  The flight took off a littl...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  ✅ Trip Verified | Once again a terrible busine...\n",
       "1  ✅ Trip Verified |  BA A380's are showing their...\n",
       "2  ✅ Trip Verified |   Credit to BA - Flew to Sin...\n",
       "3  ✅ Trip Verified |   The check in area for prem...\n",
       "4  ✅ Trip Verified |  The flight took off a littl..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a dataframe of reviews.\n",
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "-6pLWlm0Dq3b",
    "outputId": "47074bce-f851-433f-cfe6-68f08554d894",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified | Once again a terrible busine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>✅ Trip Verified |  BA A380's are showing their...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified |   Credit to BA - Flew to Sin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅ Trip Verified |   The check in area for prem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified |  The flight took off a littl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3708</th>\n",
       "      <td>HKG-LHR in New Club World on Boeing 777-300 - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3709</th>\n",
       "      <td>LHR to HAM. Purser addresses all club passenge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3710</th>\n",
       "      <td>My son who had worked for British Airways urge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3711</th>\n",
       "      <td>London City-New York JFK via Shannon on A318 b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3712</th>\n",
       "      <td>SIN-LHR BA12 B747-436 First Class. Old aircraf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3713 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                reviews\n",
       "0     ✅ Trip Verified | Once again a terrible busine...\n",
       "1     ✅ Trip Verified |  BA A380's are showing their...\n",
       "2     ✅ Trip Verified |   Credit to BA - Flew to Sin...\n",
       "3     ✅ Trip Verified |   The check in area for prem...\n",
       "4     ✅ Trip Verified |  The flight took off a littl...\n",
       "...                                                 ...\n",
       "3708  HKG-LHR in New Club World on Boeing 777-300 - ...\n",
       "3709  LHR to HAM. Purser addresses all club passenge...\n",
       "3710  My son who had worked for British Airways urge...\n",
       "3711  London City-New York JFK via Shannon on A318 b...\n",
       "3712  SIN-LHR BA12 B747-436 First Class. Old aircraf...\n",
       "\n",
       "[3713 rows x 1 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the data obtained into a csv.\n",
    "#df.to_csv(\"data/BA_reviews.csv\")\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/BA_reviews.csv\",index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1YUG-aISDq3b",
    "outputId": "741f6781-0bfa-44ab-a25a-a5332587b571"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3713 entries, 0 to 3712\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   reviews  3713 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 58.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Getting the basic information of the reviews dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ylYt5-ZDq3c"
   },
   "source": [
    "Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that we should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmVb3R0cDq3n",
    "outputId": "ae810424-a61c-46b1-893c-67066e715158"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                reviews\n",
      "0      Once again a terrible business class experien...\n",
      "1       BA A380's are showing their age, hopefully t...\n",
      "2        Credit to BA - Flew to Singapore recently a...\n",
      "3        The check in area for premium classes at Ga...\n",
      "4       The flight took off a little late due to Hea...\n",
      "...                                                 ...\n",
      "3708  HKG-LHR in New Club World on Boeing 777-300 - ...\n",
      "3709  LHR to HAM. Purser addresses all club passenge...\n",
      "3710  My son who had worked for British Airways urge...\n",
      "3711  London City-New York JFK via Shannon on A318 b...\n",
      "3712  SIN-LHR BA12 B747-436 First Class. Old aircraf...\n",
      "\n",
      "[3713 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary text\n",
    "df['reviews'] = df['reviews'].str.replace(r'^(✅ Trip Verified|Not Verified) \\|', '', regex=True)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "0cMiTyUwDq3o",
    "outputId": "8ff86fd5-c29a-4ca9-cb24-d787c2e93c79"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>once again a terrible business class experien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ba a380's are showing their age, hopefully t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0   once again a terrible business class experien...\n",
       "1    ba a380's are showing their age, hopefully t..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizing the data to lowecase\n",
    "df.reviews =df['reviews'].str.lower()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5wcI12NIDq3p"
   },
   "outputs": [],
   "source": [
    "# expanding the contractions (is-nots)\n",
    "#import contractions\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#def expand(text):\n",
    "    # Expand contractions\n",
    " #   expanded_text = contractions.fix(text)\n",
    "\n",
    "  #  return expanded_text\n",
    "\n",
    "#df['reviews'] = df['reviews'].apply(expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6xp9056jCC5S"
   },
   "outputs": [],
   "source": [
    "# removing the html tags if there is any\n",
    "import re\n",
    "def remove_html(review):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', review)\n",
    "\n",
    "df['reviews'] = df['reviews'].apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3IAUkkKHCC5T"
   },
   "outputs": [],
   "source": [
    "# removing URL and @ sign if any\n",
    "def preprocess_text_removingq_URLand_atsign(text):\n",
    "    # Remove URLs\n",
    "    clean_text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r'@[^\\s]+', 'user', clean_text)\n",
    "    # Other preprocessing steps like removing punctuation, converting to lowercase, etc.\n",
    "    # ...\n",
    "    return text\n",
    "df['reviews'] = df['reviews'].apply(preprocess_text_removingq_URLand_atsign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8ihGqV4Dq3p",
    "outputId": "62f0bbfd-9159-4632-b6b2-5b800b41cace"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "#Removing Punctuations\n",
    "import string\n",
    "exclude = string.punctuation\n",
    "print(exclude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3haoKrmHDq3q",
    "outputId": "47a85a15-4db7-4415-8bdb-1d08c3a56710"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>once again a terrible business class experien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ba a380s are showing their age hopefully the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>credit to ba  flew to singapore recently an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the check in area for premium classes at ga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the flight took off a little late due to hea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0   once again a terrible business class experien...\n",
       "1    ba a380s are showing their age hopefully the...\n",
       "2     credit to ba  flew to singapore recently an...\n",
       "3     the check in area for premium classes at ga...\n",
       "4    the flight took off a little late due to hea..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing punctuations from the reviews\n",
    "def remove_punctuations(review):\n",
    "    return review.translate(str.maketrans('','',exclude))\n",
    "\n",
    "df['reviews']=df['reviews'].apply(remove_punctuations)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "DsHMJdo_Dq3q",
    "outputId": "443bde9d-c4e1-4cb7-db2b-e1ff201a9d6e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/paul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[once, again, a, terrible, business, class, ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ba, a380s, are, showing, their, age, hopefull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[credit, to, ba, flew, to, singapore, recently...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[the, check, in, area, for, premium, classes, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[the, flight, took, off, a, little, late, due,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  [once, again, a, terrible, business, class, ex...\n",
       "1  [ba, a380s, are, showing, their, age, hopefull...\n",
       "2  [credit, to, ba, flew, to, singapore, recently...\n",
       "3  [the, check, in, area, for, premium, classes, ...\n",
       "4  [the, flight, took, off, a, little, late, due,..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the tweets\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "def tokenize_text(review):\n",
    "    return word_tokenize(review)\n",
    "\n",
    "df['reviews']=df['reviews'].apply(tokenize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "1PyZDUynDq3r",
    "outputId": "fd9e9766-36ca-4d8d-cd85-743700089ae7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/paul/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>terrible business class experience ba flight d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ba a380s showing age hopefully update interior...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>credit ba flew singapore recently also back do...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>check area premium classes gatwick nice queue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flight took little late due heathrow congestio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  terrible business class experience ba flight d...\n",
       "1  ba a380s showing age hopefully update interior...\n",
       "2  credit ba flew singapore recently also back do...\n",
       "3  check area premium classes gatwick nice queue ...\n",
       "4  flight took little late due heathrow congestio..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stop words\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(review):\n",
    "    # Use list comprehension for efficient list creation\n",
    "    new_review = [word for word in review if word not in stop_words]\n",
    "    return \" \".join(new_review)\n",
    "\n",
    "# Apply the function to the 'review' column\n",
    "df['reviews']=df['reviews'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "zN9vgF61CC5V",
    "outputId": "26340854-92b8-481e-aba6-7e323db946fe"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[terrible, business, class, experience, ba, fl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[ba, a380s, showing, age, hopefully, update, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[credit, ba, flew, singapore, recently, also, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[check, area, premium, classes, gatwick, nice,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[flight, took, little, late, due, heathrow, co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  [terrible, business, class, experience, ba, fl...\n",
       "1  [ba, a380s, showing, age, hopefully, update, i...\n",
       "2  [credit, ba, flew, singapore, recently, also, ...\n",
       "3  [check, area, premium, classes, gatwick, nice,...\n",
       "4  [flight, took, little, late, due, heathrow, co..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-tokenizing the data\n",
    "df['reviews']=df['reviews'].apply(tokenize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXy1XNNxCC5V"
   },
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "id": "Kwrbc8muDq3r",
    "outputId": "fa9d3ede-0a01-49af-b53c-84f813b0adab"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "#Frequency Distribution after Removing Stopwords\n",
    "reviews = df.reviews\n",
    "# Flatten the list of tokens into a single list\n",
    "all_tokens = [review for sublist in reviews for review in sublist]\n",
    "\n",
    "# Create the frequency distribution\n",
    "freq_dist = FreqDist(all_tokens)\n",
    "\n",
    "# Get the most common words\n",
    "most_common = freq_dist.most_common(20)\n",
    "\n",
    "# Extract the words and frequencies\n",
    "words, frequencies = zip(*most_common)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(words, frequencies)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Distribution of Words')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4c_HtZjDCC5W"
   },
   "source": [
    "#### ***Observation***\n",
    "Just by looking at this, we can see there is a high occurence of\n",
    "\n",
    "* flight\n",
    "\n",
    "* ba(**British Airways**)\n",
    "\n",
    "* Service , places like Landon, food, seats and good.\n",
    "\n",
    "These words probably occur across our various sentiments and may not necessarily add any meaning, unless a majority of their occurence is associated with a particular emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ClKrPo3KCC5W"
   },
   "outputs": [],
   "source": [
    "# Assuming 'reviews' contains the list of reviews\n",
    "specific_character = '`'\n",
    "\n",
    "# Loop through reviews and print the one containing the specific character\n",
    "for i, review in enumerate(reviews):\n",
    "    if specific_character in review:\n",
    "        print(f\"Review {i + 1}: {review}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "tOmQM8-EDq3s",
    "outputId": "e6877043-cdad-4817-f3d7-7901f29f493d"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supported for TrueType fonts",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8163/2939360595.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Call the function with the DataFrame 'df'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mgenerate_word_cloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8163/2939360595.py\u001b[0m in \u001b[0;36mgenerate_word_cloud\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmin_font_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     ).generate(all_text)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Display the word cloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \"\"\"\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    619\u001b[0m         \"\"\"\n\u001b[1;32m    620\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mfont_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 self.generate_from_frequencies(dict(frequencies[:2]),\n\u001b[0m\u001b[1;32m    454\u001b[0m                                                max_font_size=self.height)\n\u001b[1;32m    455\u001b[0m                 \u001b[0;31m# find font sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    506\u001b[0m                     font, orientation=orientation)\n\u001b[1;32m    507\u001b[0m                 \u001b[0;31m# get size of resulting text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                 \u001b[0mbox_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransposed_font\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m                 \u001b[0;31m# find possible places using integral image:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                 result = occupancy.sample_position(box_size[3] + self.margin,\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/PIL/ImageDraw.py\u001b[0m in \u001b[0;36mtextbbox\u001b[0;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0mfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfont\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreeTypeFont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only supported for TrueType fonts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"RGBA\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0membedded_color\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfontmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         bbox = font.getbbox(\n",
      "\u001b[0;31mValueError\u001b[0m: Only supported for TrueType fonts"
     ]
    }
   ],
   "source": [
    "# Here we can Plot a word cloud to see some of the most common words\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_word_cloud(df):\n",
    "    # Flatten and concatenate all text data in the DataFrame\n",
    "    all_text = \" \".join(\" \".join(map(str, review)) for review in df['reviews'])\n",
    "\n",
    "    # Create the word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        width=1600,\n",
    "        height=800,\n",
    "        max_words=2000,\n",
    "        min_font_size=5\n",
    "    ).generate(all_text)\n",
    "\n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the DataFrame 'df'\n",
    "generate_word_cloud(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "AsqVbgLrDq3s",
    "outputId": "ae231ff4-8bdc-4749-ffe6-31cba760db45"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the VADER lexicon (if not already downloaded)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Create a VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate sentiment scores and labels for each review\n",
    "sentiment_scores = []\n",
    "sentiment_labels = []\n",
    "\n",
    "for review_list in df['reviews']:\n",
    "    review = \" \".join(review_list)  # Convert list of strings to a single string\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    compound_score = scores['compound']\n",
    "\n",
    "    if compound_score >= 0.05:\n",
    "        sentiment_label = \"positive\"\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment_label = \"negative\"\n",
    "    else:\n",
    "        sentiment_label = \"neutral\"\n",
    "\n",
    "    sentiment_scores.append(scores)\n",
    "    sentiment_labels.append(sentiment_label)\n",
    "\n",
    "# Add sentiment scores and labels as new columns in the DataFrame\n",
    "df['sentiment_scores'] = sentiment_scores\n",
    "df['sentiment_labels'] = sentiment_labels\n",
    "\n",
    "# Print the DataFrame with sentiment scores and labels\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ex3ibzFgDq3s",
    "outputId": "993bc71a-83b5-45d6-9aa8-e67107e38b99"
   },
   "outputs": [],
   "source": [
    "df.sentiment_labels.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 564
    },
    "id": "wc3Qd4q5CC5X",
    "outputId": "bfcb86c6-9b63-4588-8edc-b88548ca6723"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'sentiment_labels' is a list in each row, extract the first element\n",
    "df['sentiment_labels'] = df['sentiment_labels'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "# Calculate percentages of sentiment_labels\n",
    "sentiment_labels_percentages = df['sentiment_labels'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Get the order of bars sorted by their percentages in descending order\n",
    "order = sentiment_labels_percentages.index\n",
    "\n",
    "# Plotting a countplot for sentiment labels with specified order\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.countplot(x='sentiment_labels', data=df, order=order, palette='viridis')\n",
    "\n",
    "# Annotate each bar with its percentage value\n",
    "total_count = len(df['sentiment_labels'])\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width() / 2., height + 0.5,\n",
    "            f'{height/total_count*100:.2f}%', ha='center', va='center', fontsize=10, color='black')\n",
    "\n",
    "plt.title('Distribution of sentiment_labels')\n",
    "plt.xlabel('Sentiment Review')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETJMsd5ACC5X"
   },
   "source": [
    "#### ***Observation***\n",
    "Just by looking at this, we can see that about 55% of the reviews are positive, 42% of the reviews are negative and just 2% of the reviews are neutral\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuUr4zA0CC5Y"
   },
   "source": [
    "## Lemmatization\n",
    "We want to produce valid words, better suited for tasks like text analysis, sentiment analysis, and topic modeling.\n",
    "example: Lemmatizing \"running\" would result in \"run,\" and \"better\" would remain \"better.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pN8vmkp0CC5Y",
    "outputId": "215038f4-f72e-4fbb-f47c-2ccc54881aaa"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ym9b1mcGCC5Y",
    "outputId": "0bb81c16-9777-4372-ac11-58812a55e5bb"
   },
   "outputs": [],
   "source": [
    "# Create Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem = WordNetLemmatizer()\n",
    "\n",
    "# Function for lemmatization\n",
    "def lem_words(review):\n",
    "    return [word_lem.lemmatize(word) for word in review]\n",
    "\n",
    "# Assuming 'reviews' is a column containing lists of words in each review\n",
    "df['lemmatized_reviews'] = df['reviews'].apply(lem_words)\n",
    "df['lemmatized_reviews'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "zQ4HFQelCC5Y",
    "outputId": "8df5f941-1fe9-4610-ff64-83b54579f192"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kLhqOONCCC5Y",
    "outputId": "acc8d741-f40e-4b2e-c884-96caf5028064"
   },
   "outputs": [],
   "source": [
    "# Print some examples where lemmatization differs\n",
    "for i in range(10):\n",
    "    original_review = df['reviews'][i]\n",
    "    lemmatized_review = df['lemmatized_reviews'][i]\n",
    "\n",
    "    # Check for differences\n",
    "    differences = [pair for pair in zip(original_review, lemmatized_review) if pair[0] != pair[1]]\n",
    "\n",
    "    if differences:\n",
    "        print(f\"Original: {original_review}\")\n",
    "        print(f\"Lemmatized: {lemmatized_review}\")\n",
    "        print(f\"Differences: {differences}\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kweOOEIQCC5Z"
   },
   "source": [
    "As we can see from the above most of the words have been lemmatized as we wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILf3IVWTCC5Z"
   },
   "source": [
    "### Dealing with Emojis\n",
    "**Replace Emojis with Descriptions**:\n",
    "\n",
    "Replace emojis with their corresponding descriptions. This can help maintain some information about the emotional content while removing the graphical representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_Of0U69CC5Z"
   },
   "outputs": [],
   "source": [
    "# We create a function that replaces emojis in tweet text with their corresponding meanings.\n",
    "#import emoji\n",
    "\n",
    "#def replace_emojis_with_descriptions(text):\n",
    "    # Replace emojis with their descriptions (without colons)\n",
    " #   return emoji.demojize(text, delimiters=(' ', ' ')).replace(':', ' ')\n",
    "\n",
    "#df['lemmatized_reviews'] = df['lemmatized_reviews'].apply(replace_emojis_with_descriptions)\n",
    "#df['lemmatized_reviews'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WcByK3C1CC5a"
   },
   "source": [
    "### vectorization\n",
    "#### CountVectorization\n",
    "\n",
    "Techniques is used for converting text documents into numerical representations. The following code snippet creates a Bag-of-Words representation of the lemmatized reviews using the CountVectorizer from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLNVkzeDCC5a"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Join the lemmatized words into strings\n",
    "df['lemmatized_reviews_str'] = df['lemmatized_reviews'].apply(lambda x: ' '.join(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIE9pjL4CC5b"
   },
   "source": [
    "## Feature engineering\n",
    "\n",
    "### Label Encoding the Target\n",
    "\n",
    "Here we label encode the target feature to transform the values to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aor3dh53CC5b",
    "outputId": "350d0bd9-d91a-4447-e979-0fb3d9203b12"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform\n",
    "df['labels'] = le.fit_transform(df['sentiment_labels'])\n",
    "\n",
    "# View the classes\n",
    "classes = le.classes_\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 293
    },
    "id": "1gp4CvZFCC5l",
    "outputId": "2a886acd-1195-4939-ab4e-a343a518cecf"
   },
   "outputs": [],
   "source": [
    "# previewing the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "yX8zLaVHCC5m",
    "outputId": "b140a08a-d6d6-489d-90e2-9840ef97c24c"
   },
   "outputs": [],
   "source": [
    "# displaying the encoding scheme\n",
    "df[['sentiment_labels', 'labels']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HuumX7DyCC5m"
   },
   "source": [
    "#  Modeling\n",
    "* The problem at hand is a classification problem.\n",
    "* We will explore 2 models: a binary logistic regression model, a multi-class XGBoost model and MultinomialNB.\n",
    "* Model accuracy will be the metric for evaluation.\n",
    "* Justification: Accuracy to get a verdict if a tweet is positive or negative.\n",
    "\n",
    "* Accuracy of 70% will be the threshold to deem the model as successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1Zf_DApCC5m"
   },
   "source": [
    "### 1. LogisticRegression for Binary classification\n",
    "* In this section we create a base model to identify if a tweet is 'Positive' or 'Negative'.\n",
    "* LogisticRegression will be used for the classification.\n",
    "* The normal preprocessing of vectorization and train test split will be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 738
    },
    "id": "M_3G0f7LCC5m",
    "outputId": "f023cbb2-9f47-4151-cbcf-2437465aeced"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Join the lemmatized words into strings\n",
    "df['lemmatized_reviews_str'] = df['lemmatized_reviews'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "\n",
    "# Create a copy of the original data\n",
    "data_copy = df.copy()\n",
    "\n",
    "# Define the values to drop\n",
    "value_to_drop = [1]\n",
    "\n",
    "# Drop rows with the specified values in the 'labels' column\n",
    "data_copy = data_copy[~data_copy['labels'].isin(value_to_drop)]\n",
    "\n",
    "# Plot class imbalance\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='labels', data=data_copy)\n",
    "plt.title('Class Imbalance Check')\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data_copy['lemmatized_reviews_str']\n",
    "y = data_copy['labels']\n",
    "\n",
    "# Save binary and multi-class datasets\n",
    "X_copy = X.copy()\n",
    "X_copy.to_csv('binary.csv', index=False)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save binary and multi-class datasets\n",
    "X_copy = X_train.copy()\n",
    "X_copy.to_csv('binary.csv', index=False)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "log_reg.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = log_reg.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1MTcGORCC5n"
   },
   "source": [
    "### Model Summary\n",
    "\n",
    "1. **Accuracy: 0.79**\n",
    "   - The overall accuracy of the model is 79%. This indicates the proportion of correctly predicted instances among all instances.\n",
    "\n",
    "2. **Precision:**\n",
    "   - For class 0 (negative review):\n",
    "      - Precision is 73%, meaning that of all instances predicted as class 0, 73% are truly class 0.\n",
    "   - For class 2 (positive review):\n",
    "      - Precision is 82%, indicating that of all instances predicted as class 2, 82% are truly class 2.\n",
    "\n",
    "3. **Recall (Sensitivity):**\n",
    "   - For class 0:\n",
    "      - Recall is 71%, indicating that the model correctly identifies 71% of all actual instances of class 0.\n",
    "   - For class 2:\n",
    "      - Recall is 83%, meaning that the model correctly identifies 83% of all actual instances of class 2.\n",
    "\n",
    "4. **F1-Score:**\n",
    "   - The F1-score is the harmonic mean of precision and recall, providing a balance between the two.\n",
    "   - For class 0, the F1-score is 72%.\n",
    "   - For class 2, the F1-score is 82%.\n",
    "\n",
    "5. **Support:**\n",
    "   - The number of actual occurrences of each class in the test set.\n",
    "   - Class 0 has a support of 77 instances, and class 2 has a support of 119 instances.\n",
    "\n",
    "6. **Macro Average and Weighted Average:**\n",
    "   - The macro average calculates the average of precision, recall, and F1-score across classes, giving each class equal weight.\n",
    "   - The weighted average considers the number of instances for each class, providing a weighted average based on support.\n",
    "\n",
    "In summary, the model demonstrates good overall performance with decent precision, recall, and F1-score for both classes. The weighted average takes into account the class imbalance, providing a more representative evaluation metric for an imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "2waADk7hCC5n",
    "outputId": "249313f5-5b79-4194-e55c-5e1dc5fee39b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "# Assuming 'y_test' and 'y_pred' are the true labels and predicted labels, respectively\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix using ConfusionMatrixDisplay\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Class 0', 'Class 2'])\n",
    "cm_display.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TLxGUyc6CC5n"
   },
   "source": [
    "### Testing Logistic Regression Model here\n",
    "\n",
    "* We take a sample sentence, preprocess it then pass it to the model to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ss101_gpCC5o"
   },
   "outputs": [],
   "source": [
    "# sample review\n",
    "review = '✅ Trip Verified | Liked this airline! They were organized, cozy, on time & nice. Will definitely fly with them again. Internet & entertainment should be added though.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "4BfloyNdCC5o",
    "outputId": "d34b27ab-f3ed-4d6f-b3f9-84bcc941edfc"
   },
   "outputs": [],
   "source": [
    "# lowercase\n",
    "review = review.lower()\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "ATYQYVPMCC5o",
    "outputId": "69c72ac3-6bcf-4b2b-de02-cf9aa92b1e4e"
   },
   "outputs": [],
   "source": [
    "# removing punctuations\n",
    "review = remove_punctuations(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e0-J51myCC5o",
    "outputId": "fb3a0f34-5ebd-478f-c65f-bfd06c0353bb"
   },
   "outputs": [],
   "source": [
    "# tokenizing the data\n",
    "review = tokenize_text(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Wg1gDzKbCC5o",
    "outputId": "d0ab967f-cbd8-4219-99ac-1b09689d4a45"
   },
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "review = remove_stopwords(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lfcGiIkOCC5p",
    "outputId": "d2b32f77-e5fa-443b-f8a2-556e56d41932"
   },
   "outputs": [],
   "source": [
    "# retokenizing the text\n",
    "review = tokenize_text(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D-zb29OTCC5p",
    "outputId": "11d62407-4806-41ea-984a-0c2ba0e4eed0"
   },
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "review = lem_words(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "BOgN4e2VCC5p",
    "outputId": "8cd37024-52eb-435e-c8ea-4de1f63293eb"
   },
   "outputs": [],
   "source": [
    "# joining to one sentence\n",
    "review = ' '.join(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YSqUm93gCC5q",
    "outputId": "597bb93c-46be-4c9c-d831-f4e9879ff552"
   },
   "outputs": [],
   "source": [
    "# putting to list\n",
    "review = [review]\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fTOFpCIQCC5q",
    "outputId": "1172fee0-8d38-4fd8-b0cd-6987e465b56a"
   },
   "outputs": [],
   "source": [
    "# vectorizing\n",
    "review_x = vectorizer.transform(review)\n",
    "review_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NFLqCyaVCC5r",
    "outputId": "eb4262bb-670a-433d-8fc2-e2c1bda2e7de"
   },
   "outputs": [],
   "source": [
    "test_predict = log_reg.predict(review_x)\n",
    "test_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-_7by5YXCC5r"
   },
   "source": [
    "Here we can see that the model is correct at predicting the class and it can clearly state that the above review given is positive, which is true as we can see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxHPIuMqCC5r"
   },
   "source": [
    "###  Multiclass Classifier\n",
    "* Here we work with the original dataset.\n",
    "* We build a multi-class classifier.\n",
    "* MultinomialNB and XGBoost model will be tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fwUpt78gCC5s"
   },
   "source": [
    "### 2. MultinomialNB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 914
    },
    "id": "qXPo4dsnCC5s",
    "outputId": "5afc204b-ceb4-44e6-bc05-e25ecf79d42f"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df['lemmatized_reviews_str']\n",
    "y = df['labels']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer2 = CountVectorizer()\n",
    "X_train_vectorized = vectorizer2.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer2.transform(X_test)\n",
    "\n",
    "X_copy = X_train.copy()\n",
    "X_copy.to_csv('multi.csv', index=False)\n",
    "# Initialize Multinomial Naive Bayes model\n",
    "multinomialNB = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "multinomialNB.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = multinomialNB.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=multinomialNB.classes_, yticklabels=multinomialNB.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lqj1hL9lCC5s"
   },
   "source": [
    "## Observation\n",
    "\n",
    "The confusion matrix visualizes the performance of the model on each class, showing the number of true positives, true negatives, false positives, and false negatives. Note that precision and F1-score for Class 1 are 0 because there are no predicted samples for this class.\n",
    "\n",
    "The warning messages indicate that precision and F1-score are ill-defined for Class 1 due to the absence of predicted samples, and you can use the zero_division parameter to handle this behavior.\n",
    "\n",
    "Overall, we will investigate the imbalance in class distribution and consider strategies to address it, such as oversampling, undersampling, or using different evaluation metrics for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4Imhln0WCC5t",
    "outputId": "b4c7de1a-038c-41e2-8d5b-1f146573d03b"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "# Assuming X_train_vectorized, y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_vectorized, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with SMOTE and a classifier (e.g., Multinomial Naive Bayes)\n",
    "pipeline = make_pipeline(SMOTE(random_state=42), MultinomialNB())\n",
    "\n",
    "# Train the model using the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E6HpFKVDCC5t"
   },
   "source": [
    "The classification report provides a summary of the performance of the Multinomial Naive Bayes model. The interpretation of the key metrics ie:\n",
    "\n",
    "- **Precision:** Precision is the ratio of correctly predicted positive observations to the total predicted positives. It is a measure of the accuracy of the classifier when it predicts a positive class. Precision is high for class 0 and class 2, but it's 0 for class 1. This indicates that the model is not predicting any instances of class 1 correctly.\n",
    "\n",
    "- **Recall (Sensitivity):** Recall is the ratio of correctly predicted positive observations to the all observations in actual class. It is a measure of the ability of the classifier to capture all the available positive instances. Recall is reasonable for class 0 and class 2, but it's 0 for class 1. This means the model is not capturing any instances of class 1.\n",
    "\n",
    "- **F1-score:** The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall. The F1-score is reasonable for class 0 and class 2, but it's 0 for class 1.\n",
    "\n",
    "- **Support:** Support is the number of actual occurrences of the class in the specified dataset. It indicates the number of samples for each class.\n",
    "\n",
    "- **Accuracy:** Accuracy is the ratio of correctly predicted observation to the total observations. The overall accuracy of the model is 0.78.\n",
    "\n",
    "- **Macro Avg:** Macro average calculates metrics independently for each class and then takes the average. It's providing the average performance across all classes.\n",
    "\n",
    "- **Weighted Avg:** Weighted average calculates metrics for each class but takes into account the relative size of each class. It's useful when dealing with imbalanced datasets.\n",
    "\n",
    "The model is struggling with class 1, likely due to the imbalanced nature of the dataset. Further tuning or using different algorithms may be considered to improve performance, especially for minority classes. The `zero_division` parameter can be set to control warnings about precision and F1-score being ill-defined for classes with no predicted samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gCMqMCRCC5t"
   },
   "source": [
    "### 3. XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 914
    },
    "id": "picQgkugCC5t",
    "outputId": "0712a518-5d7f-4660-cef8-d88101db0814"
   },
   "outputs": [],
   "source": [
    "# Split the data into features (X) and target variable (y)\n",
    "X = df['lemmatized_reviews_str']\n",
    "y = df['labels']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train_vectorized, y_train)\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "import xgboost as xgb\n",
    "model_xgb = xgb.XGBClassifier(objective='multi:softmax', num_class=3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model_xgb.fit(X_train_resampled, y_train_resampled)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_xgb = model_xgb.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f'Accuracy: {accuracy_xgb}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Display Confusion Matrix\n",
    "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='Blues', xticklabels=model_xgb.classes_, yticklabels=model_xgb.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretations\n",
    "- **Class 0:**\n",
    "  - **Precision (Positive Predictive Value):** 0.72\n",
    "    - Of all instances predicted as Class 0, 72% were actually Class 0.\n",
    "  - **Recall (Sensitivity):** 0.70\n",
    "    - Of all instances that are actually Class 0, the model identified 70% correctly.\n",
    "  - **F1-Score:** 0.71\n",
    "    - The harmonic mean of precision and recall.\n",
    "  - **Support:** 90\n",
    "    - The number of actual occurrences of Class 0 in the test set.\n",
    "\n",
    "\n",
    "- **Accuracy:** 0.72\n",
    "  - The ratio of correctly predicted instances to the total instances. In this case, the model predicted the correct class for approximately 72% of instances.\n",
    "\n",
    "- **Macro Average:**\n",
    "  - **Macro Avg Precision:** 0.48\n",
    "  - **Macro Avg Recall:** 0.49\n",
    "  - **Macro Avg F1-Score:** 0.48\n",
    "  - The macro-average calculates the metric independently for each class and then takes the average. It treats all classes equally.\n",
    "\n",
    "- **Weighted Average:**\n",
    "  - **Weighted Avg Precision:** 0.70\n",
    "  - **Weighted Avg Recall:** 0.72\n",
    "  - **Weighted Avg F1-Score:** 0.71\n",
    "  - The weighted average considers the number of samples for each class, giving more weight to the majority class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final note on modeling\n",
    "* Despite implementing resampling techniques to address class imbalance, the model's accuracy has not improved significantly. This suggests that other factors within the dataset may be limiting predictive performance.\n",
    "\n",
    "* The binary logistic regression model performs best.\n",
    "* For Multiclass Multinomial Naive Bayes Model was better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "1. **Logistic Regression Model:**\n",
    "   - **Accuracy:** 0.79\n",
    "   - **Precision, Recall, F1-Score:** Varies per class\n",
    "   - **Comment:** The logistic regression model achieved a decent overall accuracy, but it's essential to examine class-specific metrics for a more detailed evaluation.\n",
    "\n",
    "2. **Multinomial Naive Bayes Model:**\n",
    "   - **Accuracy:** 0.75\n",
    "   - **Precision, Recall, F1-Score:** Varies per class\n",
    "   - **Comment:** The Multinomial Naive Bayes model showed lower accuracy compared to logistic regression. It's essential to investigate the class-specific metrics to understand its performance on individual classes.\n",
    "\n",
    "3. **XGBoost Model:**\n",
    "   - **Accuracy:** 0.72\n",
    "   - **Precision, Recall, F1-Score:** Varies per class\n",
    "   - **Comment:** XGBoost performance is slightly lower than logistic regression and Naive Bayes. Investigate class-specific metrics for more insights.\n",
    "\n",
    "**Common Observations:**\n",
    "- **Class Imbalance:** There are instances of class imbalance, as indicated by the warning messages in the classification reports. This can affect the interpretation of results, especially for minority classes.\n",
    "- **Undefined Metrics:** Some models show warnings about precision and F1-score being ill-defined for certain classes. This suggests that there might be issues with the class distribution in the training or testing set.\n",
    "\n",
    "**Recommendations:**\n",
    "- **Handling Imbalance:** Consider strategies to handle class imbalance, such as oversampling minority classes (e.g., using SMOTE) or adjusting class weights during model training.\n",
    "- **Fine-Tuning Models:** Experiment with hyperparameter tuning for each model to potentially improve performance.\n",
    "- **Feature Engineering:** Explore additional feature engineering techniques to improve model understanding and predictive power.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "\n",
    "1. **Common Themes:**\n",
    "   - Identified recurring themes in the reviews, including aspects like flight experience, time management, crew behavior, class distinctions, staff service, British Airways (BA) as a brand, overall service quality, and mentions of specific locations like London.\n",
    "\n",
    "2. **High-Frequency Terms:**\n",
    "   - Observed that certain words like \"flight,\" \"time,\" \"crew,\" \"class,\" \"staff,\" \"BA,\" \"service,\" \"London,\" \"food,\" \"seats,\" and \"good\" have a high occurrence. Noted the need to assess whether these terms carry emotional weight or if they are neutral descriptors.\n",
    "\n",
    "3. **Sentiment Distribution:**\n",
    "   - Discovered that approximately 55% of the reviews express positive sentiments, 42% convey negative sentiments, and only 2% are neutral. Acknowledged the polarity distribution as a crucial aspect of customer feedback.\n",
    "\n",
    "### Conclusion:\n",
    "\n",
    "1. **Customer Sentiment Overview:**\n",
    "   - Provided a snapshot of the overall sentiment landscape, highlighting the prevailing positive sentiment but acknowledging a significant portion of negative sentiments.\n",
    "\n",
    "2. **Commonly Mentioned Aspects:**\n",
    "   - Recognized the importance of understanding the context behind frequently mentioned terms, emphasizing the need to distinguish emotional significance from neutral references.\n",
    "\n",
    "### Limitations:\n",
    "\n",
    "1. **Semantic Ambiguity:**\n",
    "   - Acknowledged the challenge of interpreting sentiment in reviews due to semantic ambiguity and the potential for different emotional interpretations of similar terms.\n",
    "\n",
    "2. **Biases and Generalization:**\n",
    "   - Addressed the limitations associated with biases in sentiment analysis tools and the potential challenges of generalizing findings to the entire customer base.\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **In-depth Analysis of High-Frequency Terms:**\n",
    "   - Suggested a detailed examination of high-frequency terms to unveil their emotional nuances, providing a more nuanced understanding of customer sentiment.\n",
    "\n",
    "2. **Contextual Analysis:**\n",
    "   - Recommended a context-specific analysis to distinguish between neutral and emotionally charged occurrences of common terms, ensuring a more accurate representation of sentiment.\n",
    "\n",
    "3. **Stakeholder Engagement:**\n",
    "   - Encouraged involvement of stakeholders from British Airways in the interpretation process to provide industry expertise and context to the sentiment analysis results.\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. **Refinement of Sentiment Analysis:**\n",
    "   - Plan to refine sentiment analysis algorithms to improve accuracy, incorporating feedback from stakeholders and validating results against manual assessments.\n",
    "\n",
    "2. **Qualitative Analysis:**\n",
    "   - Consideration of qualitative methods, such as sentiment-specific surveys or interviews, to gather deeper insights into customer emotions and preferences.\n",
    "\n",
    "3. **Enhanced Topic Modeling:**\n",
    "   - Expand topic modeling to uncover specific areas within highlighted themes that require attention, allowing for more targeted and actionable insights.\n",
    "\n",
    "4. **Iterative Approach:**\n",
    "   - Adopt an iterative approach to data collection and analysis, continuously refining methodologies based on ongoing feedback and emerging trends.\n",
    "\n",
    "By addressing these findings, conclusions, limitations, recommendations, and next steps, the sentiment analysis project will evolve into a more robust and actionable tool for British Airways to enhance customer satisfaction and make informed business decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model as a pickle file\n",
    "#joblib.dump(log_reg, 'logistic_regression_model.pkl')\n",
    "#joblib.dump(multinomialNB, 'Multinomial_Naive_Bayes_Model.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "python3(learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f7924c4c56b083e0e50eadfe7ef592a7a8ef70df33a0047f82280e6be1afe15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
