{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eLaVGrhWDq3T"
   },
   "source": [
    "# Sentimental Analysis for reviews related to British Airways\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit the site you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pUvkZmf0Dq3Y"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ecfDDBBBDq3Z",
    "outputId": "74be6e79-f384-4078-cce4-0f6087e4ff2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "   ---> 100 total reviews\n",
      "Scraping page 2\n",
      "   ---> 200 total reviews\n",
      "Scraping page 3\n",
      "   ---> 300 total reviews\n",
      "Scraping page 4\n",
      "   ---> 400 total reviews\n",
      "Scraping page 5\n",
      "   ---> 500 total reviews\n",
      "Scraping page 6\n",
      "   ---> 600 total reviews\n",
      "Scraping page 7\n",
      "   ---> 700 total reviews\n",
      "Scraping page 8\n",
      "   ---> 800 total reviews\n",
      "Scraping page 9\n",
      "   ---> 900 total reviews\n",
      "Scraping page 10\n",
      "   ---> 1000 total reviews\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 10\n",
    "page_size = 100\n",
    "\n",
    "reviews = []\n",
    "\n",
    "# for i in range(1, pages + 1):\n",
    "for i in range(1, pages + 1):\n",
    "\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "    for para in parsed_content.find_all(\"div\", {\"class\": \"text_content\"}):\n",
    "        reviews.append(para.get_text())\n",
    "\n",
    "    print(f\"   ---> {len(reviews)} total reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "tm11eIIJDq3a",
    "outputId": "0a8d9310-6603-4c5d-e6eb-333731b63763"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified |  Having not flown with BA fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>✅ Trip Verified | Dear Community I feel compel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified |  Quick bag drop at First Win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅ Trip Verified |  4 Hours before takeoff we r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified |  I recently had a delay on B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  ✅ Trip Verified |  Having not flown with BA fo...\n",
       "1  ✅ Trip Verified | Dear Community I feel compel...\n",
       "2  ✅ Trip Verified |  Quick bag drop at First Win...\n",
       "3  ✅ Trip Verified |  4 Hours before takeoff we r...\n",
       "4  ✅ Trip Verified |  I recently had a delay on B..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating a dataframe of reviews.\n",
    "df = pd.DataFrame()\n",
    "df[\"reviews\"] = reviews\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "-6pLWlm0Dq3b",
    "outputId": "9b0107fb-687a-4bc5-80ee-d5f78e2d5558",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>✅ Trip Verified |  Having not flown with BA fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>✅ Trip Verified | Dear Community I feel compel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>✅ Trip Verified |  Quick bag drop at First Win...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>✅ Trip Verified |  4 Hours before takeoff we r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>✅ Trip Verified |  I recently had a delay on B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>✅ Trip Verified | I understand completely why ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>✅ Trip Verified | London to Miami. Worst long ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>✅ Trip Verified | I used avios point to upgrad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>✅ Trip Verified | Boarding was fairly quick, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>✅ Trip Verified |  Bangalore to London. Ground...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               reviews\n",
       "0    ✅ Trip Verified |  Having not flown with BA fo...\n",
       "1    ✅ Trip Verified | Dear Community I feel compel...\n",
       "2    ✅ Trip Verified |  Quick bag drop at First Win...\n",
       "3    ✅ Trip Verified |  4 Hours before takeoff we r...\n",
       "4    ✅ Trip Verified |  I recently had a delay on B...\n",
       "..                                                 ...\n",
       "995  ✅ Trip Verified | I understand completely why ...\n",
       "996  ✅ Trip Verified | London to Miami. Worst long ...\n",
       "997  ✅ Trip Verified | I used avios point to upgrad...\n",
       "998  ✅ Trip Verified | Boarding was fairly quick, t...\n",
       "999  ✅ Trip Verified |  Bangalore to London. Ground...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the data obtained into a csv.\n",
    "#df.to_csv(\"data/BA_reviews.csv\")\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/BA_reviews.csv\",index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1YUG-aISDq3b",
    "outputId": "0e39f4e1-6126-497e-e747-c255d4811689"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1000 entries, 0 to 999\n",
      "Data columns (total 1 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   reviews  1000 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 15.6+ KB\n"
     ]
    }
   ],
   "source": [
    "# Getting the basic information of the reviews dataframe\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ylYt5-ZDq3c"
   },
   "source": [
    "Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that we should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tmVb3R0cDq3n",
    "outputId": "76ee2e58-6f35-4e6a-90c1-5a8188bfba2b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               reviews\n",
      "0      Having not flown with BA for a few years, I ...\n",
      "1     Dear Community I feel compelled to share the ...\n",
      "2      Quick bag drop at First Wing but too many pa...\n",
      "3      4 Hours before takeoff we received a Mail st...\n",
      "4      I recently had a delay on British Airways fr...\n",
      "..                                                 ...\n",
      "995   I understand completely why there is air rage...\n",
      "996   London to Miami. Worst long haul business cla...\n",
      "997   I used avios point to upgrade from economy to...\n",
      "998   Boarding was fairly quick, the well turned ou...\n",
      "999    Bangalore to London. Ground experience not g...\n",
      "\n",
      "[1000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "# Remove unnecessary text\n",
    "df['reviews'] = df['reviews'].str.replace(r'^(✅ Trip Verified|Not Verified) \\|', '', regex=True)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "0cMiTyUwDq3o",
    "outputId": "cd4fc7fa-3b70-4ac3-ebcf-ee4445e83177"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>having not flown with ba for a few years, i ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dear community i feel compelled to share the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0    having not flown with ba for a few years, i ...\n",
       "1   dear community i feel compelled to share the ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizing the data to lowecase\n",
    "df.reviews =df['reviews'].str.lower()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "5wcI12NIDq3p"
   },
   "outputs": [],
   "source": [
    "# expanding the contractions (is-nots)\n",
    "#import contractions\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#def expand(text):\n",
    "    # Expand contractions\n",
    " #   expanded_text = contractions.fix(text)\n",
    "\n",
    "  #  return expanded_text\n",
    "\n",
    "#df['reviews'] = df['reviews'].apply(expand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the html tags if there is any\n",
    "import re\n",
    "def remove_html(review):\n",
    "    pattern = re.compile('<.*?>')\n",
    "    return pattern.sub(r'', review)\n",
    "\n",
    "df['reviews'] = df['reviews'].apply(remove_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing URL and @ sign if any\n",
    "def preprocess_text_removingq_URLand_atsign(text):\n",
    "    # Remove URLs\n",
    "    clean_text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
    "    text = re.sub(r'@[^\\s]+', 'user', clean_text)\n",
    "    # Other preprocessing steps like removing punctuation, converting to lowercase, etc.\n",
    "    # ...\n",
    "    return text\n",
    "df['reviews'] = df['reviews'].apply(preprocess_text_removingq_URLand_atsign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z8ihGqV4Dq3p",
    "outputId": "cf02678c-59cf-4782-e3d7-86387c7380ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "#Removing Punctuations\n",
    "import string\n",
    "exclude = string.punctuation\n",
    "print(exclude)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "3haoKrmHDq3q",
    "outputId": "14d5c8f6-98ca-430d-d9a4-493aae745fde"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>having not flown with ba for a few years i r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dear community i feel compelled to share the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quick bag drop at first wing but too many pa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 hours before takeoff we received a mail st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i recently had a delay on british airways fr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0    having not flown with ba for a few years i r...\n",
       "1   dear community i feel compelled to share the ...\n",
       "2    quick bag drop at first wing but too many pa...\n",
       "3    4 hours before takeoff we received a mail st...\n",
       "4    i recently had a delay on british airways fr..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing punctuations from the reviews\n",
    "def remove_punctuations(review):\n",
    "    return review.translate(str.maketrans('','',exclude))\n",
    "\n",
    "df['reviews']=df['reviews'].apply(remove_punctuations)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "DsHMJdo_Dq3q",
    "outputId": "9bb4b0b1-5cc0-4a28-9762-b32f66e34a35"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/paul/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[having, not, flown, with, ba, for, a, few, ye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[dear, community, i, feel, compelled, to, shar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[quick, bag, drop, at, first, wing, but, too, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4, hours, before, takeoff, we, received, a, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[i, recently, had, a, delay, on, british, airw...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  [having, not, flown, with, ba, for, a, few, ye...\n",
       "1  [dear, community, i, feel, compelled, to, shar...\n",
       "2  [quick, bag, drop, at, first, wing, but, too, ...\n",
       "3  [4, hours, before, takeoff, we, received, a, m...\n",
       "4  [i, recently, had, a, delay, on, british, airw..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize the tweets\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "def tokenize_text(review):\n",
    "    return word_tokenize(review)\n",
    "\n",
    "df['reviews']=df['reviews'].apply(tokenize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1PyZDUynDq3r",
    "outputId": "88b43b6b-d335-44c5-fe2f-8588f48fe0d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flown ba years read reviews preparation needle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dear community feel compelled share utter frus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>quick bag drop first wing many passengers use ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4 hours takeoff received mail stating cryptic ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recently delay british airways bru lhr due sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  flown ba years read reviews preparation needle...\n",
       "1  dear community feel compelled share utter frus...\n",
       "2  quick bag drop first wing many passengers use ...\n",
       "3  4 hours takeoff received mail stating cryptic ...\n",
       "4  recently delay british airways bru lhr due sta..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stop words\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(review):\n",
    "    # Use list comprehension for efficient list creation\n",
    "    new_review = [word for word in review if word not in stop_words]\n",
    "    return \" \".join(new_review)\n",
    "\n",
    "# Apply the function to the 'review' column\n",
    "df['reviews']=df['reviews'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[flown, ba, years, read, reviews, preparation,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[dear, community, feel, compelled, share, utte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[quick, bag, drop, first, wing, many, passenge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[4, hours, takeoff, received, mail, stating, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[recently, delay, british, airways, bru, lhr, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             reviews\n",
       "0  [flown, ba, years, read, reviews, preparation,...\n",
       "1  [dear, community, feel, compelled, share, utte...\n",
       "2  [quick, bag, drop, first, wing, many, passenge...\n",
       "3  [4, hours, takeoff, received, mail, stating, c...\n",
       "4  [recently, delay, british, airways, bru, lhr, ..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# re-tokenizing the data\n",
    "df['reviews']=df['reviews'].apply(tokenize_text)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 585
    },
    "id": "Kwrbc8muDq3r",
    "outputId": "99389ac9-8512-4ebc-e2de-d7feabeeaee1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtoAAAGfCAYAAABhk3VxAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAABDwklEQVR4nO3de7xt9bz/8de7m0qlqOgql0RuUSIH8YuTisq9TggpIXfnEI7i6OAcuV9DknuE3InjmjrZEZUTJcVWJzm5X0r1+f3x/S57tqy9W3vvOdbca+3X8/FYjzXnGHOO73fMOeYYn/Edn+93pKqQJEmSNF5rTLoCkiRJ0kJkoC1JkiQNwEBbkiRJGoCBtiRJkjQAA21JkiRpAAbakiRJ0gAMtCVpnkpyXpL7j2lZByX50sjzSnLbcSy7L+8PSW49ruXNssz1knw6yW+TfHQuy56hLtv1z3StSdZD0twy0Ja0SklycZI/98Bs6m/LSddrLo0EZVPrf3mSzyR50OjrquqOVfW1WS5rmQFeVX2gqv5xDNUnydeSPHna8jeoqovGsfzl8Ejg5sDNqupRozOSbNE/l5uPTHvxUqZ9Ye6qLGkhMdCWtCp6aA/Mpv4uHZ25GrUKblxVGwB3BU4FPpHkCeMuZAF/nrcEflxV10yfUVWXARcC9xuZfD/g/BmmfWN5Cl3An6ek5WSgLWle6C2NT09yAXBBn/aQJGcn+U2Sbye5y8jr75bku0l+n+QjST6c5BV93hOSfGuG5d+2P75Rktck+VlvTX57kvX6vPsnWZzkeUl+meSyJE8cWc56SY5NcklPWfhWn/bZJM+YVuYPkux/Q+teVf9bVW8AjgZenWSN/v6LkzywP941yaIkv+t1fm1/+1SQ+JveOr5bX//TkrwuyZXA0TN9JsDeSS5K8qsk/zlS7tFJ3j+yHn9rNU9yDHBf4M29vDfP8PneJMmJSa7on9NLRpb9hP6ZvSbJr5P8NMleS/tsktyht6D/Ji2VZt8+/WXAS4HH9HocMsPbv0EPqpOsCdwNeMO0absB30iyRq/nJf17PzHJTaat/yFJfgb8V5I1+zr8KslFwD7T6v2E/tn+vq/jQUtbR0nzl4G2pPlkf+CewI5J7g4cDzwFuBnwDuBTPUheB/gk8D7gpsBHgUcsRzmvBm4H7ATcFtiKFrRNuQVwkz79EOAtSTbp814D7Azcu5f9L8B1wHuBx04tIMld+/s/txz1+jiwObDDDPPeALyhqjYCbgOc1KdPtc5u3K8OnN6f3xO4qC/vmKWU9zBgF+DuwH7Ak26oglX1YuCbwBG9vCNmeNmbaJ/frYHdgccDTxyZf0/gR8CmwH8A706S6QtJsjbwaeBLfT2eAXwgyQ5VdRTw78BHej3ePUM9/hZo04Ls84GvTJu2NnAm8IT+94Be7w2AN09b3u7AHYA9gUOBh/Rl7EJLY5mq942BNwJ7VdWGtG3l7BnqJ2meM9CWtCr6ZG+h/E2ST45Mf2VVXVlVf6YFMu+oqv+uqmur6r3AVcC9+t/awOur6q9V9THgO7MpuAd0hwLP6WX9nhawHTDysr8CL+/L/hzwB2CH3ir7JOBZVfWLXq9vV9VVwCnA9km278t4HC0IvHo5PpepFJqbzjDvr8Btk2xaVX+oqjNuaFlV9aaquqZ/njN5df8Mfga8HjhwOeo6o95K/BjgyKr6fVVdDBxL+zymXFJV76yqa2knKFvQcq2nuxct4H1VVV1dVf8FfGY56vl14E79JOm+wDer6gJg05FpZ/Tv6CDgtVV1UVX9ATgSOCDXTxM5uqr+2D/PR9O2v59X1ZXAK6eVfV0ve72quqyqzptlnSXNIwbaklZF+1fVxv1v/5HpPx95fEvgeSMB+W+AbYAt+98vqqpGXn/JLMveDFgfOGtkuV/o06f837S83z/RAr5NgXWBn0xfaA+2TwIe2wPyA2kt7stjq/7/yhnmHUJrhT8/yXeSPOQGlvXzG5g//TWX0D7XlbUpsA7X/z4uYcm6Afzv1IOq+lN/uMEMy9oS+HlVXbeMZS1VD/IXA/ehtWJ/s886fWTaVOrNljPUeS2ufwIw+nltyd9/flPl/pF2snE4cFlPK7r9bOosaX4x0JY0n4wGzj8HjhkJyDeuqvWr6kPAZcBW09INth15/EdaMA1AkluMzPsV8GfgjiPLvUnvlHhDfgX8hZa6MZP30lpG9wD+NJLGMVsPA35JS6u4nqq6oKoOpKVQvBr4WE9RqOmvnXrLLMrbZuTxtixpUb/e50dLpZntsn9Fa32/5bRl/2IW9ZnuUmCbqfzuFVzWN2kB9W7At6dNuw9LAu1LZ6jzNcDlI9NG1/sy/v7zW/LCqi9W1YNorfXnA+9cjjpLmicMtCXNV+8EDk9yzzQ3TrJPkg1pLZLXAM/sHfQeDuw68t7vA3dMslOSdWmdDAHoraPvBF6XZHOAJFsl2fOGKtTfezzw2iRb9g5xuyW5UZ9/Oi1l4FiWozU7yc2THAEcRUu5uG6G1zw2yWZ93m/65GuBK3qZKzKG9T8n2STJNsCzgI/06WcD90uybe8QeOS0912+tPJ6OshJwDFJNkxyS+C5wPtnev0N+G9a0P8vSdZOG1P8ocCHl2MZ36DliF9aVb/r077Vp92Eti0BfAh4TpJbJdmAJfnffzeiSXcSbfvbuqehvHBqRv8+9+0nQlfRUo+uXY46S5onDLQlzUtVtYiWS/1m4Ne0odqe0OddDTy8P/817TL9x0fe+2Pg5cCXaSOYTB9t4wV9eWck+V1/3UwdEGfyfOAcWk74lbTW5dF97YnAnZldYPmbJH/sy9sbeFRVHb+U1z4YOC/JH2gdIw+oqr/01ItjgNN6Ksy9Zrke0PLKz6IF1p8F3g1QVafSgu4f9Pmfmfa+NwCP7KOGvHGG5T6DFiBfRPvsP0g7QVku/XveF9iL1lL+VuDxVXX+cizm67SrAKPbwNnAesBZI6krx9NOjr4B/JR25eJ6o8hM807gi7STuu8ysv3Rtofn0VrJr6R1onzactRZ0jyR66cwStLClOQEYHFVvWTC9Xg8cFhV3WeS9ZAkDc8WbUmaI0nWp7VcHjfpukiShmegLUlzoOd4X0HLX/7ghKsjSZoDpo5IkiRJA7BFW5IkSRqAgbYkSZI0gLVu+CXz06abblrbbbfdpKshSZKkBeyss876VVVtNtO8BRtob7fddixatGjS1ZAkSdICluSSpc0zdUSSJEkagIG2JEmSNAADbUmSJGkABtqSJEnSAAy0JUmSpAEYaEuSJEkDMNCWJEmSBmCgLUmSJA3AQFuSJEkagIG2JEmSNAADbUmSJGkABtqSJEnSAAy0JUmSpAGsNekKLDTbvfCzg5dx8av2GbwMSZIkrRxbtCVJkqQBGGhLkiRJAzDQliRJkgZgoC1JkiQNwEBbkiRJGoCBtiRJkjQAA21JkiRpAAbakiRJ0gAMtCVJkqQBGGhLkiRJAzDQliRJkgZgoC1JkiQNwEBbkiRJGoCBtiRJkjQAA21JkiRpAAbakiRJ0gAMtCVJkqQBGGhLkiRJAxgs0E5yfJJfJjl3ZNpHkpzd/y5Ocnafvl2SP4/Me/vIe3ZOck6SC5O8MUmGqrMkSZI0LmsNuOwTgDcDJ05NqKrHTD1Ocizw25HX/6SqdpphOW8DDgPOAD4HPBj4/PirK0mSJI3PYC3aVfUN4MqZ5vVW6UcDH1rWMpJsAWxUVadXVdGC9v3HXFVJkiRp7CaVo31f4PKqumBk2q2SfC/J15Pct0/bClg88prFfdqMkhyWZFGSRVdcccX4ay1JkiTN0qQC7QO5fmv2ZcC2VXU34LnAB5NsBMyUj11LW2hVHVdVu1TVLpttttlYKyxJkiQtjyFztGeUZC3g4cDOU9Oq6irgqv74rCQ/AW5Ha8HeeuTtWwOXzl1tJUmSpBUziRbtBwLnV9XfUkKSbJZkzf741sD2wEVVdRnw+yT36nndjwdOmUCdJUmSpOUy5PB+HwJOB3ZIsjjJIX3WAfx9J8j7AT9I8n3gY8DhVTXVkfKpwLuAC4Gf4IgjkiRJmgcGSx2pqgOXMv0JM0w7GTh5Ka9fBNxprJWTJEmSBuadISVJkqQBGGhLkiRJAzDQliRJkgZgoC1JkiQNwEBbkiRJGoCBtiRJkjQAA21JkiRpAAbakiRJ0gAMtCVJkqQBGGhLkiRJAzDQliRJkgZgoC1JkiQNwEBbkiRJGoCBtiRJkjQAA21JkiRpAAbakiRJ0gAMtCVJkqQBGGhLkiRJAzDQliRJkgZgoC1JkiQNwEBbkiRJGoCBtiRJkjQAA21JkiRpAAbakiRJ0gAMtCVJkqQBGGhLkiRJAzDQliRJkgZgoC1JkiQNwEBbkiRJGoCBtiRJkjQAA21JkiRpAAbakiRJ0gAGC7STHJ/kl0nOHZl2dJJfJDm7/+09Mu/IJBcm+VGSPUem75zknD7vjUkyVJ0lSZKkcRmyRfsE4MEzTH9dVe3U/z4HkGRH4ADgjv09b02yZn/924DDgO3730zLlCRJklYpgwXaVfUN4MpZvnw/4MNVdVVV/RS4ENg1yRbARlV1elUVcCKw/yAVliRJksZoEjnaRyT5QU8t2aRP2wr4+chrFvdpW/XH06fPKMlhSRYlWXTFFVeMu96SJEnSrM11oP024DbATsBlwLF9+kx517WM6TOqquOqapeq2mWzzTZbyapKkiRJK25OA+2quryqrq2q64B3Arv2WYuBbUZeujVwaZ++9QzTJUmSpFXanAbaPed6ysOAqRFJPgUckORGSW5F6/R4ZlVdBvw+yb36aCOPB06ZyzpLkiRJK2KtoRac5EPA/YFNkywGjgLun2QnWvrHxcBTAKrqvCQnAT8ErgGeXlXX9kU9lTaCyXrA5/ufJEmStEobLNCuqgNnmPzuZbz+GOCYGaYvAu40xqpJkiRJg/POkJIkSdIADLQlSZKkARhoS5IkSQMw0JYkSZIGYKAtSZIkDcBAW5IkSRqAgbYkSZI0AANtSZIkaQAG2pIkSdIADLQlSZKkARhoS5IkSQMw0JYkSZIGYKAtSZIkDcBAW5IkSRqAgbYkSZI0AANtSZIkaQAG2pIkSdIADLQlSZKkARhoS5IkSQMw0JYkSZIGYKAtSZIkDcBAW5IkSRqAgbYkSZI0AANtSZIkaQAG2pIkSdIADLQlSZKkARhoS5IkSQMw0JYkSZIGYKAtSZIkDcBAW5IkSRqAgbYkSZI0AANtSZIkaQCDBdpJjk/yyyTnjkz7zyTnJ/lBkk8k2bhP3y7Jn5Oc3f/ePvKenZOck+TCJG9MkqHqLEmSJI3LkC3aJwAPnjbtVOBOVXUX4MfAkSPzflJVO/W/w0emvw04DNi+/01fpiRJkrTKGSzQrqpvAFdOm/alqrqmPz0D2HpZy0iyBbBRVZ1eVQWcCOw/QHUlSZKksZpkjvaTgM+PPL9Vku8l+XqS+/ZpWwGLR16zuE+bUZLDkixKsuiKK64Yf40lSZKkWZpIoJ3kxcA1wAf6pMuAbavqbsBzgQ8m2QiYKR+7lrbcqjquqnapql0222yzcVdbkiRJmrW15rrAJAcDDwH26OkgVNVVwFX98VlJfgLcjtaCPZpesjVw6dzWWJIkSVp+c9qineTBwAuAfavqTyPTN0uyZn98a1qnx4uq6jLg90nu1UcbeTxwylzWWZIkSVoRg7VoJ/kQcH9g0ySLgaNoo4zcCDi1j9J3Rh9h5H7Ay5NcA1wLHF5VUx0pn0obwWQ9Wk73aF63JEmStEoaLNCuqgNnmPzupbz2ZODkpcxbBNxpjFWTJEmSBuedISVJkqQBGGhLkiRJAzDQliRJkgZgoC1JkiQNwEBbkiRJGoCBtiRJkjQAA21JkiRpAAbakiRJ0gAMtCVJkqQBGGhLkiRJAzDQliRJkgZgoC1JkiQNwEBbkiRJGsBak66Axme7F3520OVf/Kp9Bl2+JEnSQjKrFu0kdxq6IpIkSdJCMtvUkbcnOTPJ05JsPGSFJEmSpIVgVoF2Vd0HOAjYBliU5INJHjRozSRJkqR5bNadIavqAuAlwAuA3YE3Jjk/ycOHqpwkSZI0X802R/suSV4H/A/w/4CHVtUd+uPXDVg/SZIkaV6a7agjbwbeCbyoqv48NbGqLk3ykkFqJkmSJM1jsw209wb+XFXXAiRZA1i3qv5UVe8brHaSJEnSPDXbHO0vA+uNPF+/T5MkSZI0g9kG2utW1R+mnvTH6w9TJUmSJGn+m22g/cckd596kmRn4M/LeL0kSZK0WpttjvazgY8mubQ/3wJ4zCA1kiRJkhaAWQXaVfWdJLcHdgACnF9Vfx20ZpIkSdI8NtsWbYB7ANv199wtCVV14iC1kiRJkua5WQXaSd4H3AY4G7i2Ty7AQFuSJEmawWxbtHcBdqyqGrIykiRJ0kIx21FHzgVuMWRFJEmSpIVkti3amwI/THImcNXUxKrad5BaSZIkSfPcbAPto4eshCRJkrTQzCp1pKq+DlwMrN0ffwf47rLek+T4JL9Mcu7ItJsmOTXJBf3/JiPzjkxyYZIfJdlzZPrOSc7p896YJMu5jpIkSdKcm1WgneRQ4GPAO/qkrYBP3sDbTgAePG3aC4GvVNX2wFf6c5LsCBwA3LG/561J1uzveRtwGLB9/5u+TEmSJGmVM9vOkE8H/gH4HUBVXQBsvqw3VNU3gCunTd4PeG9//F5g/5HpH66qq6rqp8CFwK5JtgA2qqrT+4gnJ468R5IkSVplzTbQvqqqrp56kmQt2jjay+vmVXUZQP8/FaxvBfx85HWL+7St+uPp0yVJkqRV2mwD7a8neRGwXpIHAR8FPj3GesyUd13LmD7zQpLDkixKsuiKK64YW+UkSZKk5TXbQPuFwBXAOcBTgM8BL1mB8i7v6SD0/7/s0xcD24y8bmvg0j596xmmz6iqjquqXapql80222wFqidJkiSNx2xHHbmuqt5ZVY+qqkf2xyuSOvIp4OD++GDglJHpByS5UZJb0To9ntnTS36f5F59tJHHj7xHkiRJWmXNahztJD9lhpSNqrr1Mt7zIeD+wKZJFgNHAa8CTkpyCPAz4FF9OeclOQn4IXAN8PSqurYv6qm0EUzWAz7f/yRJkqRV2mxvWLPLyON1aQHyTZf1hqo6cCmz9ljK648Bjplh+iLgTrOrpiRJkrRqmG3qyP+N/P2iql4P/L9hqyZJkiTNX7NNHbn7yNM1aC3cGw5SI0mSJGkBmG3qyLEjj6+h3Y790WOvjSRJkrRAzCrQrqoHDF0RSZIkaSGZberIc5c1v6peO57qSJIkSQvD8ow6cg/aeNcADwW+wfVvmy5JkiSpm22gvSlw96r6PUCSo4GPVtWTh6qYJEmSNJ/N9hbs2wJXjzy/Gthu7LWRJEmSFojZtmi/DzgzySdod4h8GHDiYLWSJEmS5rnZjjpyTJLPA/ftk55YVd8brlqSJEnS/Dbb1BGA9YHfVdUbgMVJbjVQnSRJkqR5b1aBdpKjgBcAR/ZJawPvH6pSkiRJ0nw32xbthwH7An8EqKpL8RbskiRJ0lLNNtC+uqqK1hGSJDcerkqSJEnS/DfbQPukJO8ANk5yKPBl4J3DVUuSJEma325w1JEkAT4C3B74HbAD8NKqOnXgukmSJEnz1g0G2lVVST5ZVTsDBteSJEnSLMw2deSMJPcYtCaSJEnSAjLbO0M+ADg8ycW0kUdCa+y+y1AVkyRJkuazZQbaSbatqp8Be81RfSRJkqQF4YZatD8J3L2qLklyclU9Yg7qJEmSJM17N5SjnZHHtx6yIpIkSdJCckOBdi3lsSRJkqRluKHUkbsm+R2tZXu9/hiWdIbcaNDaSZIkSfPUMgPtqlpzrioiSZIkLSSzHUdbkiRJ0nIw0JYkSZIGYKAtSZIkDcBAW5IkSRqAgbYkSZI0AANtSZIkaQAG2pIkSdIADLQlSZKkAcx5oJ1khyRnj/z9Lsmzkxyd5Bcj0/ceec+RSS5M8qMke851nSVJkqTldUO3YB+7qvoRsBNAkjWBXwCfAJ4IvK6qXjP6+iQ7AgcAdwS2BL6c5HZVde1c1luSJElaHpNOHdkD+ElVXbKM1+wHfLiqrqqqnwIXArvOSe0kSZKkFTTpQPsA4EMjz49I8oMkxyfZpE/bCvj5yGsW92l/J8lhSRYlWXTFFVcMU2NJkiRpFiYWaCdZB9gX+Gif9DbgNrS0ksuAY6deOsPba6ZlVtVxVbVLVe2y2WabjbfCkiRJ0nKYZIv2XsB3q+pygKq6vKqurarrgHeyJD1kMbDNyPu2Bi6d05pKkiRJy2mSgfaBjKSNJNliZN7DgHP7408BByS5UZJbAdsDZ85ZLSVJkqQVMOejjgAkWR94EPCUkcn/kWQnWlrIxVPzquq8JCcBPwSuAZ7uiCOSJEla1U0k0K6qPwE3mzbtcct4/THAMUPXS5IkSRqXiQTaWni2e+FnB13+xa/aZ9DlS5Ikjdukh/eTJEmSFiQDbUmSJGkABtqSJEnSAAy0JUmSpAEYaEuSJEkDMNCWJEmSBmCgLUmSJA3AQFuSJEkagIG2JEmSNAADbUmSJGkABtqSJEnSAAy0JUmSpAEYaEuSJEkDMNCWJEmSBmCgLUmSJA3AQFuSJEkagIG2JEmSNAADbUmSJGkABtqSJEnSAAy0JUmSpAEYaEuSJEkDWGvSFZBWxnYv/OzgZVz8qn0GL0OSJC08tmhLkiRJAzDQliRJkgZgoC1JkiQNwEBbkiRJGoCdIaUVZEdMSZK0LLZoS5IkSQMw0JYkSZIGYKAtSZIkDcBAW5IkSRrARALtJBcnOSfJ2UkW9Wk3TXJqkgv6/01GXn9kkguT/CjJnpOosyRJkrQ8Jtmi/YCq2qmqdunPXwh8paq2B77Sn5NkR+AA4I7Ag4G3JllzEhWWJEmSZmtVSh3ZD3hvf/xeYP+R6R+uqquq6qfAhcCuc189SZIkafYmFWgX8KUkZyU5rE+7eVVdBtD/b96nbwX8fOS9i/u0v5PksCSLkiy64oorBqq6JEmSdMMmdcOaf6iqS5NsDpya5PxlvDYzTKuZXlhVxwHHAeyyyy4zvkZaCIa+WY43ypEkaeVNpEW7qi7t/38JfIKWCnJ5ki0A+v9f9pcvBrYZefvWwKVzV1tJkiRp+c15oJ3kxkk2nHoM/CNwLvAp4OD+soOBU/rjTwEHJLlRklsB2wNnzm2tJUmSpOUzidSRmwOfSDJV/ger6gtJvgOclOQQ4GfAowCq6rwkJwE/BK4Bnl5V106g3pIkSdKszXmgXVUXAXedYfr/AXss5T3HAMcMXDVJkiRpbFal4f0kSZKkBcNAW5IkSRqAgbYkSZI0gEmNoy1pnprUGN5Dl7ussiVJWhG2aEuSJEkDsEVbkm6AremSpBVhi7YkSZI0AFu0JWkVNqmceEnSyjPQliTNyI6vkrRyDLQlSeomGeR79UJaeAy0JUlaza2OVy+8cqK5YGdISZIkaQAG2pIkSdIADLQlSZKkARhoS5IkSQOwM6QkSdIccoSZ1YeBtiRJ0mpidRxhZpJMHZEkSZIGYKAtSZIkDcBAW5IkSRqAgbYkSZI0AANtSZIkaQAG2pIkSdIADLQlSZKkARhoS5IkSQMw0JYkSZIGYKAtSZIkDcBAW5IkSRqAgbYkSZI0AANtSZIkaQAG2pIkSdIADLQlSZKkAcx5oJ1kmyRfTfI/Sc5L8qw+/egkv0hydv/be+Q9Rya5MMmPkuw513WWJEmSltdaEyjzGuB5VfXdJBsCZyU5tc97XVW9ZvTFSXYEDgDuCGwJfDnJ7arq2jmttSRJkrQc5rxFu6ouq6rv9se/B/4H2GoZb9kP+HBVXVVVPwUuBHYdvqaSJEnSiptojnaS7YC7Af/dJx2R5AdJjk+ySZ+2FfDzkbctZtmBuSRJkjRxEwu0k2wAnAw8u6p+B7wNuA2wE3AZcOzUS2d4ey1lmYclWZRk0RVXXDH+SkuSJEmzNJFAO8natCD7A1X1cYCquryqrq2q64B3siQ9ZDGwzcjbtwYunWm5VXVcVe1SVbtsttlmw62AJEmSdAMmMepIgHcD/1NVrx2ZvsXIyx4GnNsffwo4IMmNktwK2B44c67qK0mSJK2ISYw68g/A44Bzkpzdp70IODDJTrS0kIuBpwBU1XlJTgJ+SBux5OmOOCJJkqRV3ZwH2lX1LWbOu/7cMt5zDHDMYJWSJEmSxsw7Q0qSJEkDMNCWJEmSBmCgLUmSJA3AQFuSJEkagIG2JEmSNAADbUmSJGkABtqSJEnSAAy0JUmSpAEYaEuSJEkDMNCWJEmSBmCgLUmSJA3AQFuSJEkagIG2JEmSNAADbUmSJGkABtqSJEnSAAy0JUmSpAEYaEuSJEkDMNCWJEmSBmCgLUmSJA3AQFuSJEkagIG2JEmSNAADbUmSJGkABtqSJEnSAAy0JUmSpAEYaEuSJEkDMNCWJEmSBmCgLUmSJA3AQFuSJEkagIG2JEmSNAADbUmSJGkABtqSJEnSAAy0JUmSpAHMm0A7yYOT/CjJhUleOOn6SJIkScsyLwLtJGsCbwH2AnYEDkyy42RrJUmSJC3dvAi0gV2BC6vqoqq6GvgwsN+E6yRJkiQt1XwJtLcCfj7yfHGfJkmSJK2SUlWTrsMNSvIoYM+qenJ//jhg16p6xrTXHQYc1p/uAPxoTiu6YjYFfmXZq0XZq+M6r65lr47rvLqWvTqu8+pa9uq4zqtz2cvjllW12Uwz1prrmqygxcA2I8+3Bi6d/qKqOg44bq4qNQ5JFlXVLpa98MteHdd5dS17dVzn1bXs1XGdV9eyV8d1Xp3LHpf5kjryHWD7JLdKsg5wAPCpCddJkiRJWqp50aJdVdckOQL4IrAmcHxVnTfhakmSJElLNS8CbYCq+hzwuUnXYwCTTHWx7NWjXMtefcq17NWnXMtefcq17HlsXnSGlCRJkuab+ZKjLUmSJM0rBtqSJEmaF5Jk0nVYHgbaWvCSHJRkzUnXQ5M133bO812Stfp/jzMaRJL7Jdlz0vXQnFsf5s++ZV5UcnUxXzaaIY07GEqyBfBvwIPHudxxm77eQwWFkwo2k6yZZGrnuPEcl32PJJtUVa2uwfZc71uS3Az4bJJbV9V1q9q+bSFuB6vSOs1FXZLcDXgLcHaSDYYub74Y97Ekzbr98Xors6xxSLIdcFqSu6yK+5aZrPIVXF308cEPSbJekp2SvGTAslaZHfKoJKneOzfJrkn+IcmNVmaZVXUZ8FDgF+Oo4xCmrfdNAIYICqeVs0aStacej7OcGcpdg/Yd7J7kKODVU62dc+RpwKlJNp5ksJ1k96nvd67rUFXX9XLvnmTbqQPngH5Nu//Be5NsPekD4tTn3Q/S1ACjACTZbK5PIkfKHv1tb3NDrx+wHvdLsuYQn++0ctYFNgL+CNwbeOpcBIFJnpjkCUOXs6KmbQcPTbLuynwX/Xdzf2CXJAcBr13ZY/LK6Ot3MfB+4LgkO0x63zIbq3TlVidVdTXwZ+C3wIeBk4YoZ9oP8ZFJDkmy81Rr4ySN1Ot5wH8CRwCnJLn9Si73vKo6e+VrOH7Tvo/nACcmOSXJjcd9sBop55nA23tZD5gKwobSl/9r4DXAY4F3VdU1Q5YJrRW9l/9E4OfA+yfcsv1M4I29TnMy3FOS+yZ5cn98BHAK8DLg7Uk2HKjM9O/8g8DvgPdNOtju3/k+wFuS3HG0ruNYfv9NnQC8Ockrx7HM5TFtH/KuJDed6zokeRbwCmDbkWlj/76TPAJ4b1V9nZZCcBLwkar685An8En+BTiUdgI5On2Vabiato//N67/XSx3Pfvy1qAdj18BfLGqrhpPbZfP6LESOBO4knYif/tVPdheZSu2Ohn5AXwM+D6wOXB5n7fODK9bYSM/xGcDz6Ldzv7VwJOSbLKyy19ZSXYFdq+q3YFz+uQfT7BKgxr5PvYB9gWeB/wF+HCSLcddXpJDezkvo21nTxh3GTPpB8UvAz8Ebplk6zko81qAJI+j/Z7uAHx1roPtJHv01rZHA+tMBb5z5Drg35O8GLglcF/gaNpB6l1DBNv9s30Ibfzbr9GC7Y8lueWkDohJ7gkcC7yoqs5LcuOpA/fK1ifJAcB+tJPIPwJ3Xvkar1A9nkDbxh5XVVcmudlcNaAkeRBwEPDQqvppkh16a+p14/yd9e31EOBTSXYCzgZOBT7Yv89rhgi2k2wL7Az8A3B5kv2SvArm7qR5tpLcG3gccP+q+nGSXfqJ7nLVc+p7q6qv0PbbPwX+mmTzsVd6FkaOlc8CXg58tNfrg0nuuCoH26tkpVY3fWe/SVX9paruQTsQXpDkDlV1dZLbJlljJS8BrTHyeGdgN2B32oHhpsAdgYPSL23PlRl2wr8CzkzyBuB+tB33df3AvSD1IOBJwH9V1YVV9RjgMlqr47gD0hvTdsKPBq6ipSutM+TOM8ld+wHy+bSTukcA/5hm9/TL+QOVfV/gxcBLge2B7wFfmotgOy0vfXNaIPBJ4BnAe4GbJLnFUOX2stfogcdpwD7AwcAOwM9oaVTHAouBkzJMfutDgDdU1X8CBwBfAE5Iss3QV1CWYlvgm8D/9lbfT9D2M2uMoT5/Bl4APB7YDngY/C2HeDAzbLvr0U5uduutr18DXpRkqwHKnt7n4mbA/wD36C36HwHOTU/XGmPRVwP/TfuM/wM4vKr2ph03/hv+difpsXV+74HrocA9aFct3kRLp9g3yX+Oq5wVNcN2cB1wPrB/ktfRrl5+OSNXcmazzL5/vH0/yXgxcCTtZPIf+2tuPURj0Ax1uV2S3Ucm3Rk4sqreQ/teTqalkew4oX3LDTLQnpAkN++tmCTZCzg5LWXgDlX1RlpA8q3eGvdRWmvcChvJ0dyKdmb6YuA+tNzZXWktfocCj52rs8KpH3N/fHCSu/RZdwHuBPxTVf01yZOAlyXZbC7qNbQZdoy/oQU9O/WdOlV1GPAH4NgVPWgsJYjcCvg6sGtV7d1TOA4BnjjOg9NIHZ4JvJV28vivwA/68/8HvId2QB5nedPX+ZfAGcBvq+q6nkbyV9pva9xBwHQ3q6pf0k6iTqe1KB9F+809aKhC++/qun6gvBewiBbs7gg8oaquqapfAK+ntQiO9eS6fwdr037D0IKjT9GCsZOSrDvkCc60utwtydtpJzt3pQXY19DS0n5Ca+FfWRvSrtY8qKr27PusJ9P73Ixh+X9n2r7z4f1E9nJa48kzaev2Etrvfe0xlz3a5+Jo2tWxjwNb0vYlp1fVTrTf3b3GWXZPW7iSFuieSR99oqr2By5J8uP+/NqVLWvkOHg7WqPEJ2gnqs+tqufQAs+1h2hBn61p28FBSR5bVWfQroruCnyqqnYBvsJyfBd937EXLfY4mHbS9kNag8E/9hOMM2lXxAeTlot/EPDwJPfrkzcA9u/1vBb4IrAu8KbeaLTKpPL8TVX5N8d/tBOcx9NysY9gyY/gNbQN+779dU8CjgcevBJl3Rs4oD9+Oq1F7z29rKNpl1Lp9XkbsPkEPo+n01Jmbt+fP5p2lvp6WsvbOcAdJ/29jWldM/J4L1rrwN1owc6/A8cAu428ZosxlPlY2s5qR1qL9reBN/d5hwDnATsMsK6PoAX1N+rb+rdprSsb0Fr+HgrcdqDPdl3aQXijXoc9R+Yd2qdtPdB3vAZwe+B/+zruB7yyf/b7A98CLqK1QGaIOvR6PLev563783sDFwKHjrxmzXF97rRA9u601uNt+nZ1eJ93T1rO6F2HWt+l1GkLWivkerSAc9M+/U60wGGF9ivAE/v+c2pf/W9933oHWkreD+Zin0ULqr8/8h1vAqzfH+9Nyyce+3ZOC+jPAy4Y3V+NzN+/z99mXN/lyPMdaCeq7wD+Gdh+ZN77gFuNaR237//XpO0/X047Xgd4MnAucKehv+NZ1vV5tBb9v6sPbT987tQ2MsvlbUcLpG8D/FN//8Z93r369r/HwOs09RventZI85r+u926/75e2Oc/ps9f6WPlYOsy6Qqsrn99YzmE1nv2PSPTj6R17nhAf36j/n+FDsi0y8Y/pbU6vBe4NbAH8ELapdTr+vTzgdvN0brfDFirP96CFgzcctprdqe1wj2DMQZjk/4D1uj/D6edQLyClibyyP5ZvAJ4A63FeUXLGA0496cFFO8A3t13SjcDvtS3vf8CdhzzOqYfnPahBVxH0Fr87tnLPZGBgtxe/jNpLS9f7XXYg9ZyexTtcvOpQ+6UR36zu9GGH/sX4CzgXcA6/fO/+QDl3njk8X60FsWputyOFoTdjtYi+Pgxl70PLdA8sX++zwZuSwvE3kNrCdxznGXeQH3W6/83op20P7s/XxN4YK/XQ1dw2fvRgpA30RpCnkk7sXtJ/44/BNxhDtZxF1pwtVF/vistKFmHlh52NnDnAct/A61z7SMZ2X/3fcwPGUMQOm1f9kjaCEJ36c/v3re3Z9Mbaca4btsCF9Py3QHW6p/pqbSTqs+Oe7+5EnXdrtdrzb69708LStegpV+esTzfBe04dCvaMerRfVu/bZ+3J7DB0r6jgdZvw/53FPA6YCfaCcAP+m/tx3Pxe1updZh0BVa3P5acpW3Q/z+N1urwqJHXvIx2mWqTMZX5INoZ6Qf683VoLS/H9B/lwcxRMEs7+L6oH5jWADbrO4Kpg8U6/f9mk/6uxrzetxt5vA3wjamdAy0AvaTvxLahnZ2v0PpPOzBtBTyH618peCfwsJHXbDjAum4+rR7voaVRQGtdfAMDBbrAw2nBx+1oVwwuouUL3xY4jNZ7frADJC3o+gDtZHkPYGNaYPcRWgrFWAPcqe+8H3g+ypIT2AfS8nWP6L/z02gnHrekBWjjvJKwIe1k+T79+W379v0wWnB/e+bwihStMeHM/v2v3+vzGZYEC1sD95r67JZz2VMnMFuMbG+v75/z1L5rrYHWK9P+34fWgf7JtNFsTqddsbgz7SrZLQeqx137d7427SrJh4An9Xm70YL9rcZc5jP6NvwCWkv5Yb38u9KOlU8H1h5zmQ8FvgscODLtC7T+HmM/UV7e7WDk+Vq0E57TaI0pb6M1oLytz7/Bq9Qj29Sd+37jn2npRz8b2afcq+9DbjOH63pnWqPJrrSrvkfRrnLfvn//N53N+k36b+IVWJ3+RjbmHYEf0XL6AJ5CC4AeMfLasW7M/QDxa+AxI9NOAR4ygc/hpv1AsGd/PtXaOvWDfhLweVowPujZ8hyt70b0ob9Gpp1IOzOfWueDgHf0xzdawXLWGHn8XFpK0qXAU/q0jWnB9oeBg0e3yTGu69OBz9GCj6P7tG/TguvH9h31kK3JjwOOGXl+T1r++9hTY2Yo+5D+md+CFth9ZGTeerQW0G0HLP9mtJPqu9JOYF9My1/cg9ZCdQwDXO6lpcR8FthuZNqBwKuG/sxHypsefDyu71M/QGvdO55+lXAlyli/b0+/oXXGmpq+P60l+wha48HY91lc/8R125HHx9Cumuzen78BOGzAz/mZtIDuWFoqxQa0PPf3933cr8a9jdOCrU/RGoiOYElL5uG0k8w7AVsOtL579/Ke1L/nzwxV1gpsB4/odboP7RjzfHraDEs6Iy/Psh9CO5E4vX+f/0Hr4/L8/rl/D9hvrtavP79N3+ZOpDUQ3ITWEHUccO9JfQ/LvV6TrsDq8seSlIG9aGecp9JSBx7cpx/aDwqPHrAOD6G18L2Mdqn3u8zt2enfWmRoLRPvp13a2p7WIvO9vvP+HqtI7tu4vnvaydV76UEg7eB/AnCT/vwwWjCQlT1Q923sY33nexgtLWiqtXETWivcLQZYzwNonWa27t/th/r0W7BkbPi7jmtbYuTEYtq6f4KRk5X+exv0Mi8tkD6Qdhn32bQTxbVprU2D/cYYuYxLCwSfSzuJ36FPmzqRexQtYFjpuoz8jrdkSWrKS2lXEtYd2RZOogVHQ19anqrP/Wi5qv/Qt491aCkAJ9DymL87Vb8VKOPwvpyX0YKu84Enjsx/CHPQykkLeP6L1ln+YK5/cv1o2pXLQa5OMkd9LmhXxDbvjx/Y/9+CdnffL/fnz6WNjX/wHHzmu/f92ufoaSuT/uvb+Vf7vuY7jJxA0wLT77EcaUPAzWl9R6b2G0f0bf3f+v7z+cA/9nmDN34xEtD3betwWnx0J1pD3QuYBy3Zf1uHSVdgof9NOxBuTcsnulffsB9JC7b3oAVjTx36h0w7A7627yi3m+PPIrT0iM/358/vP+Kp9X9M31lvP5f1mqN1P4jWcn8u8II+7T20/NH39p3lCp1c0C7XHtEfb0dLIfjKyPyn0obe+n9T38MA67cBLYDfmRbcf4kWZIaej80YL+1O+10dSuuYdiwtCDiB1uL14F6XHzKGTlnLqMuTaR1Z/5N2qXW0JfspvW5jDzj75/ssWorGocBH+/SX9O1sx/58L1qO+NhOXvtnezqtpekDtID2pX07ez4tEF3hTtwrUJ+9+zo/j9b35Ch6ulbfJraitfz+Xce9WSx7qjPZTrQGgeey5MTieQOv12je/eP7um1Ou4LwdVqfjqkUjuXKxV2OOsxpnwvaidJXaB2Iv8uStLMnsST98eG0GyLNSQoH7SR2vbkoaxZ1ufnUPqZv55/q38+GwKa01t7lys2nNcB8myUNMmvTrtJ8kpEr7QOu09TJ8lSD5JnAaSPzb0tLv/sSbVSyeXWle+IVWMh/feP9Z3q+Le2yxykj89ekBQf/M7WBz1G9dmeg/L0ZyrpeXmF//H6W5EgeRQu2H8iYc+xWlT9a6955/UB9GC2wfnGftxMtSFjhnvK0IGcL+okTbSSTT48GAbRc7bMYYKQLWj+D59AC+t/RW536vENplyBXKB1mKeXtC7y7Pz6YNnzdXrSTls/2A84/A6+lpUcNmZO9M21os6nt/Ku0wHO9/l2fx5g7avVybklLrboN7W6ylzASdNDSRr5LGx9/Q8bY+kPLgf4xLWXgdrTOnqfRTrYeRWtA2H2oz3yG+tyCdnJ5S9qJ/I9pQfXLGBlpgXby9fQVWP6LgOf3x+vQWtdex5Kc1Y3H/ZvqZd2OdpI21fnv4L6uz6AFunvTrowe3effbKDPd076XHD91vm30vo0jI4WtE3/rX+WdoViTjrvT/pv+rZF29d/nHY14RSWXEV6DC1FbIX6CNBOIP+VfrJGOyZ/tH/Hg51kTPveNx55/AngayPP/5XWmDHW/P85+Q4nXYGF+seSzn5b0FqyH96nf4HrjzJyAC3Q/ATtUuy8OlNbjs9jk/5/jX7gesXIvFfSgqL1J13Pgdb9UJaMenBjWoDybeCVK7nczUd2ivvT8iNf3p/vRWtBf+7I6zceYN2eQmt92Ko/fzUtyN+WFoB/nzF2hKPlIX+ZdglxC1qr1mi/g+OBT488X6FUgVnWZS/alaFvsqT1eCpf+QRafvTYg/z+Gbyi/9+kf+YX0UdIGHndv9JandcZY9lb0HJm39qfT7VAvQU4aKjPehn1udO0en2H1uK3P60z1zG0FKqb0i79L/f30Zd1vRM2WoB924G3r3vS8vpfzJKh5jbm+p1eP03LRR8qyJ6TPhdcP5B/Ku0E41W0dKedR+ZtTTuZWzAjUS3H53JLemNU/3wuYEkH3yf1z2qFv4v+2b6ClvZ2DK2RYOe+jd11Dtb1sL5tv50+IhAtLvo6LWZYxIBXJof8m9hA6wtZkh1orSkfraqT+0077pbkN7Szzg8n+RRtI3ou7XLno4Crq29dC0mS/YC3JjmSFpS9GTgjyXeq6pSqOjLJplX1p8nWdOWN3kBgxBXAPyc5tarOA76Z5CfAVkk2r3ZTkxVxE+C1SS5nyRjN/5bkpVX18iTXAY9PckRVvZnW8jk2/WYce9ECuquSPJV2M5idaDvsdWk3HTpvjMVeTbvhyEuBot2kY/TuZE8G3p/kxlX1R9qNJsYuyeG0TkifobWw75bkt9VuBLNPv2nCeuPepvv29X9JXkY72bg37bM+DvhCko2q6i1pd1J9LfCmqrp6DGVWkjuz5EYv+/Xf73v6y/6P1to6J0bu6PjCJOdX1SuS3B64rKouT3Ix7cB8YlX9rr/nkSv4fXyN1hHroCRfo12t2AD4TVX9ZQyrcz1T61ZV/53kDrR0sAOTnEz77O9Cu4HHNbTfwIur6v8GqMcBtOPSY2lB7837rKmRVvalNSBctrJlTe0zkzwFeAKtYeoXSX4HvDPJ/rQrdVtW1ctXtrz5YNrNaJ5Nix2uTfJcWvB7LfDJJF+kfTYHrMx3UVWLk/wHbZ9yV1oj4I1pAf7/rsy63JAkD6XFQgfRUkn3SLJFVT0s7S6n69JutvXzIesxlKnLnRqTJLekDW31TdooH5+n5RUdRusQ9xlai9yR/S2n0C7tvg7Yt6oG3aDnwrQdxBNoB4of0lqc7kLLbfwTrUXu5VX118nUdLym3+mSNrLKybSOKY+jHaBeQTtgHUAbHeBXK1nma2jb1our6k1pt9k9DvhCVf1bkgcB5wy1XSU5jHYpfTGtE94ltNbso4C/Vrvz5LjL/Oe+/JfROtx9jtbC803gAbQd9gOraqwnFiPl70tr8dmnqn7Wg9rH0Fr3vlxVPxuo3NHta3fa9rUbLW3jLbQ0kY/RAsO9gftX1YVjKvshtCB7Y1pL2v/RcpffRsvHfikt6PraOMqbRX1uU1U/SfIPtJFW3k076foRrd/LHWj9Fr44Fbgu5SR4tuVtSVvfh9Lu2Pqyqvr+WFZm6WUeTrsD4odoJ3V/pLUk340lJ5rPHqIeSTagBW+X0Fo1H0nbpq6lXb1anGTtce67+4n7h2jb1PdofQ+2oAXeZ9P2m0+tqrPHVeZ80H97T6EdQ46g/c7fRcvJ343WuPGzqvrpmMt9AO1q81OG3NbT7oa8C+148bYk69PuHPxPwJOr6k8r89tdFRhoj1FvxXokbUf4EdpB/060g9+ptEvp2wNfrKrP9vfcm3aJ/6Cq+sEk6j2UJA+ktbK+tqouSnJjlpxUbE/bce6wEFqyRyV5NK2T2mm0PMvP0E6+7kNrIYJ2V6uV3nkluS1tZ/sc4Niq+kDareq/A7y6qt62smXcQPnr0k6gflJVVyY5iNaqvM9Q32s/md2eFlz+K6215fm0/PBb0Q4M5w5Rdi//cOCmVfXvSdaqqmv67YqfRMudPKnGcAvoZZR/H+CoqnpQkvvS03eq6o1JtqVtD2eO68Cb5Oa0E8ZDqupHSZ5O++2uQcvX/ilwRlV9ehzlzaI+29JSYo6jpek8C/hWVb0j7Vbk+wM/qqozByh7fdpx84/jXva0cqafzN2Ttu/4PS1V5HJav4ex/8aSPI3WgfQvtLSkM6vqgX3eofQ79VW7Hfq4y546cf85Ldf+Etq29n7giqq6ctxlrmr6b/omVfWZfpXmNcCvq+pxff5zaL/xE4AvDdGY0cvZgpZ2dsmYlzvaYLARLZjfmJaKdehUHJTkS7SBA743zvInwdSRMeqXV79My6t7Gq2X9u9owy5B6+DxbGDvJGf1VsY/AXvP10sio0YuMa9B67X8TFpr9slJLukHpz8meSxtZ/2nBRhk34/WOexRVfXTJI+idSoBeF9VHZ9knZW9nD+lt1hemJaWdEyS39IOxt+mXTkZVL90/p0kayQ5hLZ9Hzjk99p3/Jck+SfaCe1LaL+xdWjBxxVDld1dQkud2KGqftSnrUFr5f3quIPskTQJkjyS1qr1boCq+maStYGD+yXWd1XVR8ZZPi1dZ+rmUj+iBbhvo+U9f6iqPtrrNnirU2/1/CXtJHZ3WgdfgJcm+VlVfZ52G+5BzOH+akvaZ/uzfjL332mpYI+ltfS9caAgeyp142E9dWM7YMd+cjPVsvpPQwTZ3Ym01uypE/fH0q4k/HtV/XmgMlc1VwFnJ9myqs5P8gngn5I8tqreX1WvS3Ij2nCiX6Ol0o3dOFKClrLcqSB7x6r6YZLv0hpIvgM8J8mHaOlZN6HdNXneM9Aev9/SevtPddY6iXaQegQt+Hw9bSik/wVYKJfBph1kN6uWJ/koWqvjQ2kjq0yt87W0y83z3rSz86lhsG5Cy7s/oqo+muRaWmrBX5O8f1xB9qiq+nSSv9JG+Lia1jHugnGXswzrAtfRxoH/n7kosKrOSvII2rjCmwzdej/iNNoQZAcn+TatNeaZtBzJsafojATZm9PSIrai3X76fX3+f/Vg++G0dIJxl//rJB8DHpDkN1V1bpIP04KufZJ8pqr+PFSQPXICvy3td3USbRzdD9N+b5+grftjk3wL+MN8vszczXQytyXtJPqEIfYhmUyfi+tZxon7gg+yp7bzqjqz/9YXJXlRVb27H0Pu31/ygap6VZJN5mtDVZLdaH3V/p22/3487SrVZ2gjRv2ello571NpwdSRwfTL21+mDUfzftrQTPeiDbm2Unm5q7J+WXkf2igIF9Bav97Xn7++qi6dYPXGalqQvR1AVV2cljf6VOD8qnpFn78v8J2hWglG6rR5q8bgrbozlT2RPLokdwL+XFU/mcMyt6DdbXVf2sn1K2vMqV89rWzbqvpw/11NjU++Ha0192VV9ZaR168/1IE3yda0S/o70xoS9qcdHI8GXlID5XCOBNl70nKET+tlvpA2tOE6VfXqJAfSWkHHni4yCf2S+r/QGmm+TTt5fxbtZO6iAcud8z4XS6nH+rTGiTPm6sR9kqYdS7aoqsuS/CMtreI/quojSR5PO7Z+fICrVnMmyTq0EbNOop28vZzWcLEn7YT5J7Tf9VBXTeacgfaAkuxEG37sLbSxR29aVYsnWqkBpfVSP5wlvdTXqqoDkmxM6+TyXeCl4760PtemB5RpnfP2pV1K/zTtrDy0z+LiqnrxRCqqwfWDBgO1MO5DG6HnRFpw/TJaytWtaZeNt6bdwONfx132UuqzEUtGJPgcbUSC44AHVdXlA5Z7T1o6wweq6ltJ7k7rk7AOrfFit4V0Aj9lLk7mZihzzvtcLKMu87oD3IpIG11kf9qdEX+b1s/pWNodhU/qJ5RfG7rBZii98WBPWpD9J1rn3o/Tfstvp51EHzPfY4TpDLQH1g8KJwP3qwWQhz1q2ln4jL3Uq3UU25KWq36TasOfzWtZ0gFuDVpv6dfRWhhvRhvx4ipaB5ZdaD3Fn18DDL+lhS9t1JjXAd+vqoN6buatWTISw9No+bRzepUsA45IkOTWtLuYvquv70m0sdhvO/KaTWkB/2tpKVrfHGcdViVDnswto8w1gCeyJHVjsM7FapI8iXbV6lFV9fMkN+0nO/ejXRV+dlV9YrK1XDlJtqEF2ofTGiDXoXVy/XhPFfraXF6ZnCsG2nMgyYZV9ftJ12OcpgXZN9RLfQfaKBtzctlxSP0Avwi4e98J3pN2++2H9xaIm9NuVHJMz89eb3XIL9Rw0sahPwE4fOqScdo4/K+pqm9MqE6DjEjQl70V7S6AF1XVL3tu9im0NIKnTnvtSg/dp7+3uqVurArSxse+kHYcvTMtGH0XrdV3N+CSIdOG5lKSu9Kuem8IbFpVt59wlQZlZ8i58YdJV2DcRoLs2fZSn/dBNkBV/SrJM4DTe4eOs2jDm+2e5LRqnUCnLoVhkK2VVVWnJHkc8MYkO9Ju+rQNMLF0iaEuXSdZs+9H/hc4J8mnquqFaTe0OD7JG6rqWSP1uK7/N8geo2pjF5/g5zq8tHGyf0DLiX9Gn3w8rR/CQcDmVfXVCVVvEFX1/bR7TewBPCvJdlV18YSrNRhbtLXCcv0bDJxFG+d1K1rHz6/SOjq8rAbspT4paeMmv5HWI38P2sgqa9A6fR4C7FlzO+qHFri0u+OdTLtN8QsX2oFppOPjbWg5yesDnwROrqpjeofMj9DSaJ42wapKY9FTo15OS4M6kJZ2mKr6Y8/P/jfa1dJ5mZM9GxnzjY9WRQbaWimrSi/1SUiyN0tysbek3aFuC+D9VfXjSdZNC1PaHSEvHiJlY1XQW65fAVxMG33gS7Qc9fdWG9JsG2CLhTK6iNTTEY8A7gE8vdrIVU+jdUI9uKrOmWgFtdIMtLVSVqVe6pPQR4Z4NXDfauMN/+3mIpJmL8m9aDf72od20noc7b4D3+yPj6s+XKY03+T6N556DLBTVR3Zn9+UNpLOjrQOzlsCv10oOdmrOwNtjcXq3Eu9X9I/mnYTEQy0peXXU0O2ADahtWr/E/AO2o2uPgZcWVVfn1wNpZWXdq+DTWmd5v+jqt7Up+9Ou3rzQ+DxHkcWjjUmXQEtGKN3BlxtgmyAqvokcJ+qus6do7RiqmpxVX2HNlTmB6rqQtpoKzsAZ1XV15NkknWUlleSe/d7TEzd0O1UWj+mM4F/7dOg3cTl88C/eBxZWBx1RGOxuvdSr6oFN7KMNCHnAE9Jshatk/Ezqupn4Ogimpc2AV6Z5A60G089jHbjqYuBzYDDk+wK3A/YayHefGl1Z6CtsfEgKGkMPkcbl39f2qX10ydcH2mFVdVnk1zNkhtPXZTkF8DPgFvRWrh/TBs8wCB7ATJHW5K0yhm5A6s3o9G8t5QbT30SeF9VnTzBqmlgtmhLklZF14JXyrQwLOXGU9sCZ0+0YhqcgbYkaZVjgK2Fpqo+0/seTN146uEL7cZT+numjkiSJM2RhX7jKV2fgbYkSZI0AMfRliRJkgZgoC1JkiQNwEBbkiRJGoCBtiRJkjQAA21JWkCSvC7Js0eefzHJu0aeH5vkuSuw3Psn+cyYqilJqwUDbUlaWL4N3BsgyRrApsAdR+bfGzjthhaSZM1BaidJqxEDbUlaWE6jB9q0APtc4PdJNklyI+AOwMZJvpfknCTH9+kkuTjJS5N8C3hUkgcnOb8/f/hUAUl2T3J2//tekg3ndhUlaX7wzpCStIBU1aVJrkmyLS3gPh3YCtgN+C3wY+BdwB5V9eMkJwJPBV7fF/GXqrpPknWBC4D/B1wIfGSkmOcDT6+q05JsAPxlDlZNkuYdW7QlaeGZatWeCrRPH3n+C+CnVfXj/tr3Avcbee9UQH37/roL+u3Q3z9t+a9N8kxg46q6ZrA1kaR5zEBbkhaeqTztO9NSR86gtWjfG/juDbz3jyOPZ7x1cFW9CngysB5wRpLbr2yFJWkhMtCWpIXnNOAhwJVVdW1VXQlsTAu23wNsl+S2/bWPA74+wzLOB26V5Db9+YFTM5LcpqrOqapXA4tord+SpGkMtCVp4TmHNtrIGdOm/baqFgNPBD6a5BzgOuDt0xdQVX8BDgM+2ztDXjIy+9lJzk3yfeDPwOeHWQ1Jmt/SUu8kSZIkjZMt2pIkSdIADLQlSZKkARhoS5IkSQMw0JYkSZIGYKAtSZIkDcBAW5IkSRqAgbYkSZI0AANtSZIkaQD/H+6FNU2RxrudAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "#Frequency Distribution after Removing Stopwords\n",
    "reviews = df.reviews\n",
    "# Flatten the list of tokens into a single list\n",
    "all_tokens = [review for sublist in reviews for review in sublist]\n",
    "\n",
    "# Create the frequency distribution\n",
    "freq_dist = FreqDist(all_tokens)\n",
    "\n",
    "# Get the most common words\n",
    "most_common = freq_dist.most_common(20)  \n",
    "\n",
    "# Extract the words and frequencies\n",
    "words, frequencies = zip(*most_common)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(words, frequencies)\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Frequency Distribution of Words')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Observation***\n",
    "Just by looking at this, we can see there is a high occurence of\n",
    "\n",
    "* flight \n",
    "\n",
    "* ba(**British Airways**) \n",
    "\n",
    "* Service , places like Landon, food, seats and good. \n",
    "\n",
    "These words probably occur across our various sentiments and may not necessarily add any meaning, unless a majority of their occurence is associated with a particular emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'reviews' contains the list of reviews\n",
    "specific_character = '.'\n",
    "\n",
    "# Loop through reviews and print the one containing the specific character\n",
    "for i, review in enumerate(reviews):\n",
    "    if specific_character in review:\n",
    "        print(f\"Review {i + 1}: {review}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 501
    },
    "id": "tOmQM8-EDq3s",
    "outputId": "59854340-c521-4492-d83b-dc2ee0b872d6"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Only supported for TrueType fonts",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8351/2939360595.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Call the function with the DataFrame 'df'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mgenerate_word_cloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_8351/2939360595.py\u001b[0m in \u001b[0;36mgenerate_word_cloud\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mmin_font_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     ).generate(all_text)\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Display the word cloud\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \"\"\"\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    619\u001b[0m         \"\"\"\n\u001b[1;32m    620\u001b[0m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mfont_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m                 self.generate_from_frequencies(dict(frequencies[:2]),\n\u001b[0m\u001b[1;32m    454\u001b[0m                                                max_font_size=self.height)\n\u001b[1;32m    455\u001b[0m                 \u001b[0;31m# find font sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[0;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[1;32m    506\u001b[0m                     font, orientation=orientation)\n\u001b[1;32m    507\u001b[0m                 \u001b[0;31m# get size of resulting text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m                 \u001b[0mbox_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextbbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfont\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransposed_font\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manchor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"lt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m                 \u001b[0;31m# find possible places using integral image:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m                 result = occupancy.sample_position(box_size[3] + self.margin,\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/PIL/ImageDraw.py\u001b[0m in \u001b[0;36mtextbbox\u001b[0;34m(self, xy, text, font, anchor, spacing, align, direction, features, language, stroke_width, embedded_color)\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0mfont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfont\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfont\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFreeTypeFont\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Only supported for TrueType fonts\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0mmode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"RGBA\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0membedded_color\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfontmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         bbox = font.getbbox(\n",
      "\u001b[0;31mValueError\u001b[0m: Only supported for TrueType fonts"
     ]
    }
   ],
   "source": [
    "# Here we can Plot a word cloud to see some of the most common words\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_word_cloud(df):\n",
    "    # Flatten and concatenate all text data in the DataFrame\n",
    "    all_text = \" \".join(\" \".join(map(str, review)) for review in df['reviews'])\n",
    "\n",
    "    # Create the word cloud\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        width=1600,\n",
    "        height=800,\n",
    "        max_words=2000,\n",
    "        min_font_size=5\n",
    "    ).generate(all_text)\n",
    "\n",
    "    # Display the word cloud\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with the DataFrame 'df'\n",
    "generate_word_cloud(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsqVbgLrDq3s"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download the VADER lexicon (if not already downloaded)\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "# Create a VADER sentiment analyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Calculate sentiment scores and labels for each review\n",
    "sentiment_scores = []\n",
    "sentiment_labels = []\n",
    "\n",
    "for review_list in df['reviews']:\n",
    "    review = \" \".join(review_list)  # Convert list of strings to a single string\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    compound_score = scores['compound']\n",
    "    \n",
    "    if compound_score >= 0.05:\n",
    "        sentiment_label = \"positive\"\n",
    "    elif compound_score <= -0.05:\n",
    "        sentiment_label = \"negative\"\n",
    "    else:\n",
    "        sentiment_label = \"neutral\"\n",
    "    \n",
    "    sentiment_scores.append(scores)\n",
    "    sentiment_labels.append(sentiment_label)\n",
    "\n",
    "# Add sentiment scores and labels as new columns in the DataFrame\n",
    "df['sentiment_scores'] = sentiment_scores\n",
    "df['sentiment_labels'] = sentiment_labels\n",
    "\n",
    "# Print the DataFrame with sentiment scores and labels\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ex3ibzFgDq3s"
   },
   "outputs": [],
   "source": [
    "df.sentiment_labels.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'sentiment_labels' is a list in each row, extract the first element\n",
    "df['sentiment_labels'] = df['sentiment_labels'].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "\n",
    "# Calculate percentages of sentiment_labels\n",
    "sentiment_labels_percentages = df['sentiment_labels'].value_counts(normalize=True) * 100\n",
    "\n",
    "# Get the order of bars sorted by their percentages in descending order\n",
    "order = sentiment_labels_percentages.index\n",
    "\n",
    "# Plotting a countplot for sentiment labels with specified order\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = sns.countplot(x='sentiment_labels', data=df, order=order, palette='viridis')\n",
    "\n",
    "# Annotate each bar with its percentage value\n",
    "total_count = len(df['sentiment_labels'])\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x() + p.get_width() / 2., height + 0.5,\n",
    "            f'{height/total_count*100:.2f}%', ha='center', va='center', fontsize=10, color='black')\n",
    "\n",
    "plt.title('Distribution of sentiment_labels')\n",
    "plt.xlabel('Sentiment Review')\n",
    "plt.ylabel('Percentage')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Observation***\n",
    "Just by looking at this, we can see that about 55% of the reviews are positive, 42% of the reviews are negative and just 2% of the reviews are neutral\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "We want to produce valid words, better suited for tasks like text analysis, sentiment analysis, and topic modeling.\n",
    "example: Lemmatizing \"running\" would result in \"run,\" and \"better\" would remain \"better.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem = WordNetLemmatizer()\n",
    "\n",
    "# Function for lemmatization\n",
    "def lem_words(review):\n",
    "    return [word_lem.lemmatize(word) for word in review]\n",
    "\n",
    "# Assuming 'reviews' is a column containing lists of words in each review\n",
    "df['lemmatized_reviews'] = df['reviews'].apply(lem_words)\n",
    "df['lemmatized_reviews'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some examples where lemmatization differs\n",
    "for i in range(10):\n",
    "    original_review = df['reviews'][i]\n",
    "    lemmatized_review = df['lemmatized_reviews'][i]\n",
    "    \n",
    "    # Check for differences\n",
    "    differences = [pair for pair in zip(original_review, lemmatized_review) if pair[0] != pair[1]]\n",
    "    \n",
    "    if differences:\n",
    "        print(f\"Original: {original_review}\")\n",
    "        print(f\"Lemmatized: {lemmatized_review}\")\n",
    "        print(f\"Differences: {differences}\")\n",
    "        print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the above most of the words have been lemmatized as we wanted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with Emojis\n",
    "**Replace Emojis with Descriptions**:\n",
    "\n",
    "Replace emojis with their corresponding descriptions. This can help maintain some information about the emotional content while removing the graphical representation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a function that replaces emojis in tweet text with their corresponding meanings.\n",
    "import emoji\n",
    "\n",
    "def replace_emojis_with_descriptions(text):\n",
    "    # Replace emojis with their descriptions (without colons)\n",
    "    return emoji.demojize(text, delimiters=(' ', ' ')).replace(':', ' ')\n",
    "\n",
    "df['lemmatized_reviews'] = df['lemmatized_reviews'].apply(replace_emojis_with_descriptions)\n",
    "df['lemmatized_reviews'].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vectorization\n",
    "#### CountVectorization\n",
    "\n",
    "Techniques is used for converting text documents into numerical representations. The following code snippet creates a Bag-of-Words representation of the lemmatized reviews using the CountVectorizer from scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Join the lemmatized words into strings\n",
    "df['lemmatized_reviews_str'] = df['lemmatized_reviews'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Create an instance of the vectorizer\n",
    "bow = CountVectorizer()\n",
    "\n",
    "# Apply the vectorizer to the lemmatized_reviews_str column\n",
    "x = bow.fit_transform(df['lemmatized_reviews_str'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking feature names\n",
    "feature_names = bow.get_feature_names_out()\n",
    "print(feature_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "### Label Encoding the Target\n",
    "\n",
    "Here we label encode the target feature to transform the values to numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Instantiate LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Fit and transform\n",
    "df['labels'] = le.fit_transform(df['sentiment_labels'])\n",
    "\n",
    "# View the classes\n",
    "classes = le.classes_\n",
    "print(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previewing the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the encoding scheme\n",
    "df[['sentiment_labels', 'labels']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Modeling\n",
    "* The problem at hand is a classification problem.\n",
    "* We will explore 2 models: a binary logistic regression model, a multi-class XGBoost model and MultinomialNB.\n",
    "* Model accuracy will be the metric for evaluation.\n",
    "* Justification: Accuracy to get a verdict if a tweet is positive or negative.\n",
    "\n",
    "* Accuracy of 70% will be the threshold to deem the model as successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. LogisticRegression for Binary classification\n",
    "* In this section we create a base model to identify if a tweet is 'Positive' or 'Negative'.\n",
    "* LogisticRegression will be used for the classification.\n",
    "* The normal preprocessing of vectorization and train test split will be implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Create a copy of the original data\n",
    "data_copy = df.copy()\n",
    "\n",
    "# Define the values to drop\n",
    "value_to_drop = [1]\n",
    "\n",
    "# Drop rows with the specified values in the 'labels' column\n",
    "data_copy = data_copy[~data_copy['labels'].isin(value_to_drop)]\n",
    "\n",
    "# Plot class imbalance\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x='labels', data=data_copy)\n",
    "plt.title('Class Imbalance Check')\n",
    "plt.xlabel('Sentiment Label')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = data_copy['lemmatized_reviews_str']\n",
    "y = data_copy['labels']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Summary\n",
    "\n",
    "1. **Accuracy: 0.79**\n",
    "   - The overall accuracy of the model is 79%. This indicates the proportion of correctly predicted instances among all instances.\n",
    "\n",
    "2. **Precision:**\n",
    "   - For class 0 (negative review):\n",
    "      - Precision is 73%, meaning that of all instances predicted as class 0, 73% are truly class 0.\n",
    "   - For class 2 (positive review):\n",
    "      - Precision is 82%, indicating that of all instances predicted as class 2, 82% are truly class 2.\n",
    "\n",
    "3. **Recall (Sensitivity):**\n",
    "   - For class 0:\n",
    "      - Recall is 71%, indicating that the model correctly identifies 71% of all actual instances of class 0.\n",
    "   - For class 2:\n",
    "      - Recall is 83%, meaning that the model correctly identifies 83% of all actual instances of class 2.\n",
    "\n",
    "4. **F1-Score:**\n",
    "   - The F1-score is the harmonic mean of precision and recall, providing a balance between the two.\n",
    "   - For class 0, the F1-score is 72%.\n",
    "   - For class 2, the F1-score is 82%.\n",
    "\n",
    "5. **Support:**\n",
    "   - The number of actual occurrences of each class in the test set.\n",
    "   - Class 0 has a support of 77 instances, and class 2 has a support of 119 instances.\n",
    "\n",
    "6. **Macro Average and Weighted Average:**\n",
    "   - The macro average calculates the average of precision, recall, and F1-score across classes, giving each class equal weight.\n",
    "   - The weighted average considers the number of instances for each class, providing a weighted average based on support.\n",
    "\n",
    "In summary, the model demonstrates good overall performance with decent precision, recall, and F1-score for both classes. The weighted average takes into account the class imbalance, providing a more representative evaluation metric for an imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "# Assuming 'y_test' and 'y_pred' are the true labels and predicted labels, respectively\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix using ConfusionMatrixDisplay\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=['Class 0', 'Class 2'])\n",
    "cm_display.plot(cmap='Blues', values_format='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Logistic Regression Model here\n",
    "\n",
    "* We take a sample sentence, preprocess it then pass it to the model to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample review\n",
    "review = '✅ Trip Verified | Liked this airline! They were organized, cozy, on time & nice. Will definitely fly with them again. Internet & entertainment should be added though.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase\n",
    "review = review.lower()\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing punctuations\n",
    "review = remove_punctuations(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing the data\n",
    "review = tokenize_text(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing stopwords\n",
    "review = remove_stopwords(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retokenizing the text\n",
    "review = tokenize_text(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "review = lem_words(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining to one sentence\n",
    "review = ' '.join(review)\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting to list\n",
    "review = [review]\n",
    "review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing\n",
    "review_x = vectorizer.transform(review)\n",
    "review_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predict = model.predict(review_x)\n",
    "test_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that the model is correct at predicting the class and it can clearly state that the above review given is positive, which is true as we can see. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Multiclass Classifier\n",
    "* Here we work with the original dataset.\n",
    "* We build a multi-class classifier.\n",
    "* MultinomialNB and XGBoost model will be tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. MultinomialNB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = df['lemmatized_reviews_str']\n",
    "y = df['labels']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Vectorize the text data\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "# Initialize Multinomial Naive Bayes model\n",
    "model = MultinomialNB()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_vectorized, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test_vectorized)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Display Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=model.classes_, yticklabels=model.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation\n",
    "\n",
    "The confusion matrix visualizes the performance of the model on each class, showing the number of true positives, true negatives, false positives, and false negatives. Note that precision and F1-score for Class 1 are 0 because there are no predicted samples for this class.\n",
    "\n",
    "The warning messages indicate that precision and F1-score are ill-defined for Class 1 due to the absence of predicted samples, and you can use the zero_division parameter to handle this behavior.\n",
    "\n",
    "Overall, we will investigate the imbalance in class distribution and consider strategies to address it, such as oversampling, undersampling, or using different evaluation metrics for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import make_pipeline\n",
    "\n",
    "# Assuming X_train_vectorized, y_train are your training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_vectorized, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a pipeline with SMOTE and a classifier (e.g., Multinomial Naive Bayes)\n",
    "pipeline = make_pipeline(SMOTE(random_state=42), MultinomialNB())\n",
    "\n",
    "# Train the model using the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "# Evaluate the model\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification report provides a summary of the performance of the Multinomial Naive Bayes model. The interpretation of the key metrics ie:\n",
    "\n",
    "- **Precision:** Precision is the ratio of correctly predicted positive observations to the total predicted positives. It is a measure of the accuracy of the classifier when it predicts a positive class. Precision is high for class 0 and class 2, but it's 0 for class 1. This indicates that the model is not predicting any instances of class 1 correctly.\n",
    "\n",
    "- **Recall (Sensitivity):** Recall is the ratio of correctly predicted positive observations to the all observations in actual class. It is a measure of the ability of the classifier to capture all the available positive instances. Recall is reasonable for class 0 and class 2, but it's 0 for class 1. This means the model is not capturing any instances of class 1.\n",
    "\n",
    "- **F1-score:** The F1-score is the harmonic mean of precision and recall. It provides a balance between precision and recall. The F1-score is reasonable for class 0 and class 2, but it's 0 for class 1.\n",
    "\n",
    "- **Support:** Support is the number of actual occurrences of the class in the specified dataset. It indicates the number of samples for each class.\n",
    "\n",
    "- **Accuracy:** Accuracy is the ratio of correctly predicted observation to the total observations. The overall accuracy of the model is 0.78.\n",
    "\n",
    "- **Macro Avg:** Macro average calculates metrics independently for each class and then takes the average. It's providing the average performance across all classes.\n",
    "\n",
    "- **Weighted Avg:** Weighted average calculates metrics for each class but takes into account the relative size of each class. It's useful when dealing with imbalanced datasets.\n",
    "\n",
    "The model is struggling with class 1, likely due to the imbalanced nature of the dataset. Further tuning or using different algorithms may be considered to improve performance, especially for minority classes. The `zero_division` parameter can be set to control warnings about precision and F1-score being ill-defined for classes with no predicted samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_vectorized, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the XGBoost model\n",
    "model_xgb = xgb.XGBClassifier(objective='multi:softmax', num_class=3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f'Accuracy: {accuracy_xgb}')\n",
    "\n",
    "# Display classification report and confusion matrix\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "# Display Confusion Matrix\n",
    "conf_matrix_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix_xgb, annot=True, fmt='d', cmap='Blues', xticklabels=model_xgb.classes_, yticklabels=model_xgb.classes_)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretations\n",
    "- **Class 0:**\n",
    "  - **Precision (Positive Predictive Value):** 0.72\n",
    "    - Of all instances predicted as Class 0, 72% were actually Class 0.\n",
    "  - **Recall (Sensitivity):** 0.70\n",
    "    - Of all instances that are actually Class 0, the model identified 70% correctly.\n",
    "  - **F1-Score:** 0.71\n",
    "    - The harmonic mean of precision and recall.\n",
    "  - **Support:** 90\n",
    "    - The number of actual occurrences of Class 0 in the test set.\n",
    "\n",
    "\n",
    "- **Accuracy:** 0.72\n",
    "  - The ratio of correctly predicted instances to the total instances. In this case, the model predicted the correct class for approximately 72% of instances.\n",
    "\n",
    "- **Macro Average:**\n",
    "  - **Macro Avg Precision:** 0.48\n",
    "  - **Macro Avg Recall:** 0.49\n",
    "  - **Macro Avg F1-Score:** 0.48\n",
    "  - The macro-average calculates the metric independently for each class and then takes the average. It treats all classes equally.\n",
    "\n",
    "- **Weighted Average:**\n",
    "  - **Weighted Avg Precision:** 0.70\n",
    "  - **Weighted Avg Recall:** 0.72\n",
    "  - **Weighted Avg F1-Score:** 0.71\n",
    "  - The weighted average considers the number of samples for each class, giving more weight to the majority class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f7924c4c56b083e0e50eadfe7ef592a7a8ef70df33a0047f82280e6be1afe15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
